{
  "parsedContent": {
    "content": "# Configuration System\n\nThis document describes the centralized configuration system for folder-mcp.\n\n## Configuration File Structure\n\nThe main configuration is stored in `config.yaml` at the root of the project:\n\n```yaml\n# folder-mcp Configuration\n# This file contains all configuration settings for the folder-mcp project\n\n# Embedding Model Configuration\nembeddings:\n  defaultModel: \"nomic-v1.5\"\n  ollamaApiUrl: \"http://127.0.0.1:11434\"\n  batchSize: 32\n  timeoutMs: 30000\n  \n  # Available embedding models\n  models:\n    nomic-v1.5:\n      name: \"Nomic Embed v1.5\"\n      description: \"High-quality general-purpose embedding model with 768 dimensions\"\n      transformersModel: \"nomic-ai/nomic-embed-text-v1.5\"\n      ollamaModel: \"nomic-embed-text\"\n      dimensions: 768\n      maxTokens: 2048\n      isDefault: true\n    \n    mxbai-large:\n      name: \"MixedBread AI Large\"\n      description: \"Large embedding model with excellent performance (1024 dimensions)\"\n      transformersModel: \"mixedbread-ai/mxbai-embed-large-v1\"\n      ollamaModel: \"mxbai-embed-large\"\n      dimensions: 1024\n      maxTokens: 512\n    \n    # ... more models\n\n# Cache Configuration\ncache:\n  defaultCacheDir: \"~/.cache/folder-mcp\"  # User cache directory\n  maxCacheSize: \"10GB\"                    # Maximum cache size (future feature)\n  cleanupIntervalHours: 24                # How often to clean old entries\n\n# Text Processing Configuration\nprocessing:\n  defaultChunkSize: 1000        # Default size for text chunking\n  defaultOverlap: 200           # Default overlap between chunks\n  maxConcurrentOperations: 10   # Maximum parallel operations\n\n# API Configuration\napi:\n  defaultPort: 3000     # Default port for API services\n  timeoutMs: 30000      # API request timeout in milliseconds\n  rateLimitRpm: 1000    # Rate limiting (requests per minute)\n\n# Logging Configuration\nlogging:\n  level: \"info\"         # debug, info, warn, error\n  format: \"json\"        # json, text\n  file: \"logs/app.log\"  # Log file path (optional)\n\n# Development Configuration\ndevelopment:\n  enableDebugOutput: false  # Show detailed debug information\n  mockOllamaApi: false      # Use mock Ollama API for testing\n  skipGpuDetection: false   # Skip GPU detection for testing\n```\n```\n\n## Configuration Module\n\nThe configuration is accessed through `src/config.ts` which provides:\n\n### Main Configuration Access\n```typescript\nimport { \n  getConfig, \n  getEmbeddingConfig, \n  getCacheConfig,\n  getLoggingConfig,\n  getDevelopmentConfig\n} from './config.js';\n\n// Get full configuration\nconst config = getConfig();\n\n// Get specific sections\nconst embeddingConfig = getEmbeddingConfig();\nconst cacheConfig = getCacheConfig();\nconst loggingConfig = getLoggingConfig();\nconst devConfig = getDevelopmentConfig();\n```\n\n### Embedding Model Configuration\n```typescript\nimport { \n  getModelConfig, \n  getDefaultModelConfig, \n  listAvailableModels \n} from './config.js';\n\n// Get specific model configuration\nconst model = getModelConfig('mxbai-large');\n\n// Get default model\nconst defaultModel = getDefaultModelConfig();\n\n// List all available models\nconst models = listAvailableModels();\n```\n\n## Available Embedding Models\n\n| Model Key | Name | Dimensions | Description |\n|-----------|------|------------|-------------|\n| `nomic-v1.5` | Nomic Embed v1.5 | 768 | High-quality general-purpose (default) |\n| `mxbai-large` | MixedBread AI Large | 1024 | Large model with excellent performance |\n| `all-minilm` | All-MiniLM-L6-v2 | 384 | Lightweight and fast |\n| `bge-small` | BGE Small | 384 | BAAI general embedding, small version |\n| `gte-base` | GTE Base | 768 | General Text Embeddings model |\n\n## Configuration Sections\n\n### Embeddings\n- **defaultModel**: Which embedding model to use by default\n- **ollamaApiUrl**: URL for Ollama API (GPU acceleration)\n- **batchSize**: Default batch size for processing multiple texts\n- **timeoutMs**: Request timeout in milliseconds\n- **models**: Available embedding model configurations\n\n### Cache\n- **defaultCacheDir**: Where to store cached models and data\n- **maxCacheSize**: Maximum cache size (for future cleanup)\n- **cleanupIntervalHours**: How often to clean old cache entries\n\n### Processing\n- **defaultChunkSize**: Default size for text chunking\n- **defaultOverlap**: Default overlap between chunks\n- **maxConcurrentOperations**: Maximum parallel operations\n\n### API\n- **defaultPort**: Default port for API services\n- **timeoutMs**: API request timeout\n- **rateLimitRpm**: Rate limiting (requests per minute)\n\n### Logging\n- **level**: Log level (debug, info, warn, error)\n- **format**: Log format (json, text)\n- **file**: Optional log file path\n\n### Development\n- **enableDebugOutput**: Show detailed debug information\n- **mockOllamaApi**: Use mock Ollama API for testing\n- **skipGpuDetection**: Skip GPU detection for testing\n\n## Extending Configuration\n\nTo add new configuration sections:\n\n1. **Add to config.yaml**: Add your new section to the YAML file\n2. **Define interface**: Add TypeScript interface in `src/config.ts`\n3. **Add getter function**: Create a getter function like `getYourConfig()`\n4. **Update AppConfig**: Add your section to the main `AppConfig` interface\n\nExample:\n```typescript\n// In src/config.ts\nexport interface YourConfig {\n  setting1: string;\n  setting2: number;\n}\n\nexport interface AppConfig {\n  embeddings: EmbeddingConfig;\n  cache: CacheConfig;\n  processing: ProcessingConfig;\n  api: ApiConfig;\n  logging: LoggingConfig;\n  development: DevelopmentConfig;\n  your: YourConfig;  // Add this\n}\n\nexport function getYourConfig(): YourConfig {\n  return loadConfig().your;\n}\n```\n\n## Benefits of YAML Configuration\n\n- **Human-readable**: Easy to read and edit\n- **Comments supported**: Add explanatory comments\n- **Type safety**: TypeScript interfaces provide compile-time validation\n- **Hierarchical**: Natural nested structure\n- **Extensible**: Easy to add new sections\n\n## Migration from Old System\n\nThe old embedding-specific config file (`src/embeddings/config.ts`) has been removed. All imports should now use:\n\n```typescript\n// Old\nimport { getModelConfig } from './config.js';\n\n// New\nimport { getModelConfig } from '../config.js';\n```\n\nThe API remains the same for backward compatibility. The configuration has been moved from:\n- **From**: `src/embeddings/config.ts` (TypeScript)\n- **To**: `config.yaml` (YAML) + `src/config.ts` (loader)\n",
    "type": "md",
    "originalPath": "docs\\Legacy\\CONFIGURATION.md",
    "metadata": {
      "type": "md",
      "originalPath": "docs\\Legacy\\CONFIGURATION.md",
      "size": 6350,
      "lastModified": "2025-06-10T13:40:57.332Z",
      "lines": 214,
      "encoding": "utf-8"
    }
  },
  "chunks": [
    {
      "content": "# Configuration System\n\nThis document describes the centralized configuration system for folder-mcp.\n\n## Configuration File Structure\n\nThe main configuration is stored in `config.yaml` at the root of the project:\n\n```yaml\n# folder-mcp Configuration\n# This file contains all configuration settings for the folder-mcp project\n\n# Embedding Model Configuration\nembeddings:\n  defaultModel: \"nomic-v1.5\"\n  ollamaApiUrl: \"http://127.0.0.1:11434\"\n  batchSize: 32\n  timeoutMs: 30000\n\n# Available embedding models\n  models:\n    nomic-v1.5:\n      name: \"Nomic Embed v1.5\"\n      description: \"High-quality general-purpose embedding model with 768 dimensions\"\n      transformersModel: \"nomic-ai/nomic-embed-text-v1.5\"\n      ollamaModel: \"nomic-embed-text\"\n      dimensions: 768\n      maxTokens: 2048\n      isDefault: true\n\nmxbai-large:\n      name: \"MixedBread AI Large\"\n      description: \"Large embedding model with excellent performance (1024 dimensions)\"\n      transformersModel: \"mixedbread-ai/mxbai-embed-large-v1\"\n      ollamaModel: \"mxbai-embed-large\"\n      dimensions: 1024\n      maxTokens: 512\n\n# ... more models\n\n# Cache Configuration\ncache:\n  defaultCacheDir: \"~/.cache/folder-mcp\"  # User cache directory\n  maxCacheSize: \"10GB\"                    # Maximum cache size (future feature)\n  cleanupIntervalHours: 24                # How often to clean old entries\n\n# Text Processing Configuration\nprocessing:\n  defaultChunkSize: 1000        # Default size for text chunking\n  defaultOverlap: 200           # Default overlap between chunks\n  maxConcurrentOperations: 10   # Maximum parallel operations\n\n# API Configuration\napi:\n  defaultPort: 3000     # Default port for API services\n  timeoutMs: 30000      # API request timeout in milliseconds\n  rateLimitRpm: 1000    # Rate limiting (requests per minute)\n\n# Logging Configuration\nlogging:\n  level: \"info\"         # debug, info, warn, error\n  format: \"json\"        # json, text\n  file: \"logs/app.log\"  # Log file path (optional)\n\n# Development Configuration\ndevelopment:\n  enableDebugOutput: false  # Show detailed debug information\n  mockOllamaApi: false      # Use mock Ollama API for testing\n  skipGpuDetection: false   # Skip GPU detection for testing\n```\n```\n\n## Configuration Module\n\nThe configuration is accessed through `src/config.ts` which provides:\n\n### Main Configuration Access\n```typescript\nimport { \n  getConfig, \n  getEmbeddingConfig, \n  getCacheConfig,\n  getLoggingConfig,\n  getDevelopmentConfig\n} from './config.js';\n\n// Get full configuration\nconst config = getConfig();\n\n// Get specific sections\nconst embeddingConfig = getEmbeddingConfig();\nconst cacheConfig = getCacheConfig();\nconst loggingConfig = getLoggingConfig();\nconst devConfig = getDevelopmentConfig();\n```\n\n### Embedding Model Configuration\n```typescript\nimport { \n  getModelConfig, \n  getDefaultModelConfig, \n  listAvailableModels \n} from './config.js';\n\n// Get specific model configuration\nconst model = getModelConfig('mxbai-large');\n\n// Get default model\nconst defaultModel = getDefaultModelConfig();\n\n// List all available models\nconst models = listAvailableModels();\n```\n\n## Available Embedding Models",
      "startPosition": 0,
      "endPosition": 3135,
      "tokenCount": 448,
      "chunkIndex": 0,
      "metadata": {
        "sourceFile": "docs\\Legacy\\CONFIGURATION.md",
        "sourceType": "md",
        "totalChunks": 2,
        "hasOverlap": false,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\Legacy\\CONFIGURATION.md",
          "size": 6350,
          "lastModified": "2025-06-10T13:40:57.332Z",
          "lines": 214,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "\n\n| Model Key | Name | Dimensions | Description |\n|-----------|------|------------|-------------|\n| `nomic-v1.5` | Nomic Embed v1.5 | 768 | High-quality general-purpose (default) |\n| `mxbai-large` | MixedBread AI Large | 1024 | Large model with excellent performance |\n| `all-minilm` | All-MiniLM-L6-v2 | 384 | Lightweight and fast |\n| `bge-small` | BGE Small | 384 | BAAI general embedding, small version |\n| `gte-base` | GTE Base | 768 | General Text Embeddings model |\n\n## Configuration Sections\n\n### Embeddings\n- **defaultModel**: Which embedding model to use by default\n- **ollamaApiUrl**: URL for Ollama API (GPU acceleration)\n- **batchSize**: Default batch size for processing multiple texts\n- **timeoutMs**: Request timeout in milliseconds\n- **models**: Available embedding model configurations\n\n### Cache\n- **defaultCacheDir**: Where to store cached models and data\n- **maxCacheSize**: Maximum cache size (for future cleanup)\n- **cleanupIntervalHours**: How often to clean old cache entries\n\n### Processing\n- **defaultChunkSize**: Default size for text chunking\n- **defaultOverlap**: Default overlap between chunks\n- **maxConcurrentOperations**: Maximum parallel operations\n\n### API\n- **defaultPort**: Default port for API services\n- **timeoutMs**: API request timeout\n- **rateLimitRpm**: Rate limiting (requests per minute)\n\n### Logging\n- **level**: Log level (debug, info, warn, error)\n- **format**: Log format (json, text)\n- **file**: Optional log file path\n\n### Development\n- **enableDebugOutput**: Show detailed debug information\n- **mockOllamaApi**: Use mock Ollama API for testing\n- **skipGpuDetection**: Skip GPU detection for testing\n\n## Extending Configuration\n\nTo add new configuration sections:\n\n1. **Add to config.yaml**: Add your new section to the YAML file\n2. **Define interface**: Add TypeScript interface in `src/config.ts`\n3. **Add getter function**: Create a getter function like `getYourConfig()`\n4. **Update AppConfig**: Add your section to the main `AppConfig` interface\n\nExample:\n```typescript\n// In src/config.ts\nexport interface YourConfig {\n  setting1: string;\n  setting2: number;\n}\n\nexport interface AppConfig {\n  embeddings: EmbeddingConfig;\n  cache: CacheConfig;\n  processing: ProcessingConfig;\n  api: ApiConfig;\n  logging: LoggingConfig;\n  development: DevelopmentConfig;\n  your: YourConfig;  // Add this\n}\n\nexport function getYourConfig(): YourConfig {\n  return loadConfig().your;\n}\n```\n\n## Benefits of YAML Configuration\n\n- **Human-readable**: Easy to read and edit\n- **Comments supported**: Add explanatory comments\n- **Type safety**: TypeScript interfaces provide compile-time validation\n- **Hierarchical**: Natural nested structure\n- **Extensible**: Easy to add new sections\n\n## Migration from Old System\n\n\n\nThe old embedding-specific config file (`src/embeddings/config.ts`) has been removed. All imports should now use:\n\n```typescript\n// Old\nimport { getModelConfig } from './config.js';\n\n// New\nimport { getModelConfig } from '../config.js';\n```\n\nThe API remains the same for backward compatibility. The configuration has been moved from:\n- **From**: `src/embeddings/config.ts` (TypeScript)\n- **To**: `config.yaml` (YAML) + `src/config.ts` (loader)",
      "startPosition": 3135,
      "endPosition": 6329,
      "tokenCount": 557,
      "chunkIndex": 1,
      "metadata": {
        "sourceFile": "docs\\Legacy\\CONFIGURATION.md",
        "sourceType": "md",
        "totalChunks": 2,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\Legacy\\CONFIGURATION.md",
          "size": 6350,
          "lastModified": "2025-06-10T13:40:57.332Z",
          "lines": 214,
          "encoding": "utf-8"
        }
      }
    }
  ],
  "processedAt": "2025-06-18T20:41:34.845Z"
}