{
  "parsedContent": {
    "content": "# Product Requirements Document (PRD): MCP Endpoint Redesign for Folder-MCP v2.0\n\n## üß≠ Overview\n\nThis document outlines the second-generation redesign of the MCP (Mesh Control Protocol) endpoints for the `folder-mcp` tool. The tool turns any local folder into an AI-accessible knowledge base by indexing its content and exposing it through a streamlined, LLM-friendly API.\n\n### üîÑ What's Changing and Why\n\nThe original version exposed too many low-level, granular endpoints that required LLMs to orchestrate multi-step workflows inefficiently. The new design consolidates and simplifies the API into a smaller set of intent-driven, semantically meaningful endpoints. The goals are to:\n\n- Reduce planning complexity for LLMs\n- Improve speed by reducing endpoint chatter\n- Align responses with token limits and LLM context boundaries\n- Allow flexible exploration of local documents through structured access\n\n### üìå Migration Requirements\n\n1. **REMOVE ALL OLD ENDPOINTS**: Delete every endpoint previously registered, including `search_documents`, `get_document_content`, `query_table`, etc.\n2. **CLEAN UP ALL OLD TESTS**: Go over all existing test cases and remove or refactor anything dependent on deprecated endpoints.\n   - ‚ö†Ô∏è Tests for infrastructure (e.g., embedding mechanism, file watching) should remain.\n   - ‚úÖ Focus Phase 1 on clearing technical debt to accelerate new development.\n3. **REPLACE WITH NEW TESTS BASED ON USER STORIES**: Each new endpoint must be covered by real-world LLM-driven user stories (see below).\n\n---\n\n## üö® **Safety Framework**\n\n### **Backup Strategy**\n```powershell\n# Create backup branch before starting\ngit checkout -b backup/pre-mcp-endpoint-redesign\ngit add -A\ngit commit -m \"Backup before MCP endpoint redesign implementation\"\n\n# Create implementation branch  \ngit checkout -b feature/mcp-endpoint-redesign\n```\n\n### **Rollback Plan**\n```powershell\n# If major issues arise, return to backup\ngit checkout backup/pre-mcp-endpoint-redesign \ngit checkout -b feature/mcp-endpoint-redesign-retry\n```\n\n### **Validation Commands**\n```powershell\n# Run after each major task completion\nnpm run build        # Must compile without errors\nnpm test             # All tests must pass\ngit status           # Verify clean working state\n```\n\n---\n\n## ‚úÖ New Endpoint Specification\n\nEach endpoint includes:\n\n- Purpose\n- Input/Output spec\n- User story\n- Metadata (optional but recommended)\n\n### üîç `search`\n\n**Purpose**: Search documents or chunks using semantic similarity or regex patterns.\n\n**When is it triggered?** When the user asks to find information, documents, or patterns in their knowledge base.\n\n**What is it supposed to do?** Return relevant documents/chunks with rich context and location information to enable precise retrieval.\n\n**Input**:\n\n```json\n{\n  \"query\": \"Q1 financial results\",  // Natural language OR regex pattern\n  \"mode\": \"semantic\" | \"regex\",\n  \"scope\": \"documents\" | \"chunks\",\n  \"filters\": { \"folder\": \"optional-folder-name\", \"fileType\": \"pdf\" },\n  \"max_tokens\": 2000,  // Optional, defaults to 2000\n  \"continuation_token\": \"...\"  // Optional, for pagination\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"results\": [\n      {\n        \"document_id\": \"abc\",\n        \"preview\": \"Revenue grew by 15%...\",\n        \"score\": 0.92,\n        \"location\": {\n          \"page\": 23,\n          \"section\": \"Financial Results\",\n          \"sheet\": null,\n          \"slide\": null\n        },\n        \"context\": {\n          \"before\": \"Q2 showed steady growth...\",\n          \"after\": \"This trend is expected...\"\n        },\n        \"metadata\": {\n          \"document_type\": \"pdf\",\n          \"total_pages\": 94\n        }\n      }\n    ],\n    \"token_count\": 1850\n  },\n  \"status\": {\n    \"code\": \"success\",\n    \"message\": \"SEARCH_COMPLETED\"\n  },\n  \"continuation\": {\n    \"has_more\": true,\n    \"token\": \"eyJvZmZzZXQiOjEwfQ==\"\n  }\n}\n```\n\n**User Stories**:\n\n**Story 1: \"Find last month's sales performance and analyze trends\"**\n```typescript\n// Step 1: Search for sales data\nawait search({\n  query: \"sales performance october 2024\",\n  mode: \"semantic\",\n  scope: \"documents\"\n});\n// Returns: [{\"document_id\": \"sales_report_oct.xlsx\", \"preview\": \"Total sales: $1.2M...\", \"location\": {\"sheet\": \"Summary\"}}, \n//          {\"document_id\": \"board_deck_oct.pptx\", \"preview\": \"Sales exceeded target by 15%...\", \"location\": {\"slide\": 8}}]\n\n// Step 2: Get detailed data from spreadsheet\nawait getSheetData({\n  document_id: \"sales_report_oct.xlsx\",\n  sheet_name: \"Summary\"\n});\n\n// Step 3: Get presentation insights\nawait getSlides({\n  document_id: \"board_deck_oct.pptx\",\n  slide_numbers: \"8-12\"\n});\n```\n\n**Story 2: \"Find all vendor contracts and check their expiration dates\"**\n```typescript\n// Step 1: Search for contract patterns\nawait search({\n  query: \"\\\\b(contract|agreement)\\\\b.*\\\\b(vendor|supplier)\\\\b\",\n  mode: \"regex\",\n  scope: \"chunks\"\n});\n// Returns: Multiple hits with exact page locations\n\n// Step 2: For each result, get the full context\nawait getPages({\n  document_id: \"acme_vendor_agreement.pdf\",\n  page_range: \"1,15\"  // First and signature pages\n});\nawait getPages({\n  document_id: \"supplies_contract_2024.docx\"\n  // Get all pages to find dates\n});\n```\n\n**Metadata**:\n\n```json\n{\n  \"description\": \"Unified search with semantic or pattern matching\",\n  \"recommendedUsage\": \"Use mode='semantic' for concepts, mode='regex' for patterns\"\n}\n```\n\n---\n\n### üìÑ `get_document_outline`\n\n**Purpose**: Get structural overview of any document type without content.\n\n**When is it triggered?** When the user needs to understand what's in a document before retrieving specific parts, or when dealing with large documents.\n\n**What is it supposed to do?** Return document structure (pages, sheets, slides, bookmarks) without the actual content, enabling targeted retrieval.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"report.pdf\"\n}\n```\n\n**Output varies by document type**:\n\n**For PDFs:**\n```json\n{\n  \"type\": \"pdf\",\n  \"total_pages\": 94,\n  \"bookmarks\": [\n    {\"title\": \"Introduction\", \"page\": 1},\n    {\"title\": \"Financial Overview\", \"page\": 23}\n  ],\n  \"file_size\": \"2.4MB\"\n}\n```\n\n**For Excel/Sheets:**\n```json\n{\n  \"type\": \"xlsx\",\n  \"sheets\": [\n    {\"name\": \"Revenue\", \"rows\": 2847, \"columns\": 12},\n    {\"name\": \"Summary\", \"rows\": 4, \"columns\": 8},\n    {\"name\": \"Charts\", \"rows\": 0, \"columns\": 0}\n  ],\n  \"total_rows\": 2851,\n  \"file_size\": \"1.2MB\"\n}\n```\n\n**For PowerPoint:**\n```json\n{\n  \"type\": \"pptx\",\n  \"total_slides\": 45,\n  \"slides\": [\n    {\"number\": 1, \"title\": \"Q4 Business Review\"},\n    {\"number\": 2, \"title\": \"Agenda\"},\n    {\"number\": 3, \"title\": null}\n  ],\n  \"file_size\": \"15.3MB\"\n}\n```\n\n**User Story: \"What's in this 100-page report? I need the financial section\"**\n```typescript\n// Step 1: Get outline to see what's in the document\nawait getDocumentOutline(\"annual_report_2024.pdf\");\n// Returns: {\"type\": \"pdf\", \"total_pages\": 94, \"bookmarks\": [\n//   {\"title\": \"Executive Summary\", \"page\": 1},\n//   {\"title\": \"Financial Overview\", \"page\": 23},\n//   {\"title\": \"Risk Factors\", \"page\": 45}\n// ]}\n\n// Step 2: Extract just the financial section\nawait getPages({\n  document_id: \"annual_report_2024.pdf\",\n  page_range: \"23-44\"  // Financial section based on bookmarks\n});\n```\n\n---\n\n### üìÑ `get_document_data`\n\n**Purpose**: Fetch content in raw, structured, or summarized form for general documents.\n\n**When is it triggered?** When the user needs document content that doesn't fit specialized formats (not paginated, not spreadsheets, not slides).\n\n**What is it supposed to do?** Return text content from simple documents like .txt, .md, .html, .xml files in various formats.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"abc\",\n  \"format\": \"raw\" | \"chunks\" | \"metadata\",\n  \"section\": \"optional-section-id\",\n  \"max_tokens\": 2000,  // Optional\n  \"continuation_token\": \"...\"  // Optional\n}\n```\n\n**Output**: Depends on `format`:\n\n- `raw`: `{ \"content\": \"Full document text...\" }`\n- `chunks`: `[{ \"chunk_id\": \"123\", \"content\": \"...\", \"metadata\": {...} }]`\n- `metadata`: `{ \"title\": \"...\", \"author\": \"...\", \"created\": \"...\", \"pages\": 10 }`\n\n**Implementation Note**: Metadata should be extracted and cached immediately after creating/updating/deleting embeddings when a file changes. This ensures `format=\"metadata\"` responses are instant without re-parsing documents.\n\n**User Story: \"Research company's remote work policy\"**\n```typescript\n// Step 1: Search for remote work policies\nawait search({\n  query: \"remote work policy guidelines WFH\",\n  mode: \"semantic\",\n  scope: \"chunks\"\n});\n\n// Step 2: Get full context for relevant documents\nawait getDocumentData({\n  document_id: \"employee_handbook_2024.pdf\",\n  format: \"chunks\"\n});\n\n// Step 3: Get the full remote work policy document\nawait getDocumentData({\n  document_id: \"remote_work_policy_v3.docx\",\n  format: \"raw\"\n});\n```\n\n**Supported formats**: `.txt`, `.md`, `.html`, `.xml`\n\n---\n\n### üìÅ `list_folders` / `list_documents`\n\n**Purpose**: Explore folder structure and contents.\n\n**When is it triggered?** When the user wants to browse their knowledge base, find documents in specific folders, or understand the organization structure.\n\n**What is it supposed to do?** Return folder hierarchies and document listings to enable navigation through the knowledge base.\n\n**Input**:\n\n- `list_folders()` has no input\n- `list_documents(folder)`:\n\n```json\n{ \n  \"folder\": \"name\",\n  \"max_tokens\": 2000,  // Optional for large folders\n  \"continuation_token\": \"...\"  // Optional\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"documents\": [\n      {\"name\": \"Q1_Report.pdf\", \"document_id\": \"abc\", \"modified\": \"2024-05-02\"}\n    ],\n    \"token_count\": 450\n  },\n  \"status\": { \"code\": \"success\" },\n  \"continuation\": { \"has_more\": false }\n}\n```\n\n**User Story: \"Find all Q4 financial documents by department\"**\n```typescript\n// Step 1: Explore folder structure\nawait listFolders();\n// Returns: [\"Finance\", \"Sales\", \"Marketing\", \"Operations\", ...]\n\n// Step 2: Check Finance folder for Q4 docs\nawait listDocuments({ folder: \"Finance/2024/Q4\" });\n\n// Step 3: Check other departments\nawait listDocuments({ folder: \"Sales/Reports/Q4\" });\n```\n\n---\n\n### üìä `get_sheet_data`\n\n**Purpose**: Extract tabular data from spreadsheet files.\n\n**When is it triggered?** When the user needs data from Excel, CSV, or similar files for analysis, calculations, or data extraction.\n\n**What is it supposed to do?** Return structured tabular data with headers and rows, ready for analysis or processing.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"financials.xlsx\",\n  \"sheet_name\": \"Revenue\",  // Optional, error for CSV if provided\n  \"cell_range\": \"A1:D10\",   // Optional\n  \"max_tokens\": 2000,       // Optional\n  \"continuation_token\": \"...\" // Optional\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"headers\": [\"Month\", \"Revenue\"],\n    \"rows\": [[\"Jan\", \"$1000\"], ...],\n    \"token_count\": 1850\n  },\n  \"status\": {\n    \"code\": \"success\"\n  },\n  \"continuation\": {\n    \"has_more\": true,\n    \"token\": \"eyJyb3ciOjEwMX0=\"\n  },\n  \"actions\": [\n    {\n      \"id\": \"CONTINUE\",\n      \"description\": \"Get next batch of rows\",\n      \"params\": {\"continuation_token\": \"$CONTINUATION_TOKEN\"}\n    }\n  ]\n}\n```\n\n**CSV Note**: Returns error if sheet_name is provided: \"CSV files don't have multiple sheets. Omit sheet_name parameter.\"\n\n**User Story: \"Analyze customer churn across sources\"**\n```typescript\n// Step 1: Search for churn data\nawait search({\n  query: \"customer churn retention analysis\",\n  mode: \"semantic\",\n  scope: \"documents\"\n});\n\n// Step 2: Get main analysis file\nawait getSheetData({\n  document_id: \"churn_analysis_2024.xlsx\",\n  sheet_name: \"Monthly_Churn\"\n});\n\n// Step 3: Get raw CSV data\nawait getSheetData({\n  document_id: \"retention_report.csv\"\n  // No sheet_name for CSV\n});\n```\n\n**Supported formats**: `.xlsx`, `.xls`, `.ods`, `.csv`\n\n---\n\n### üéØ `get_slides`\n\n**Purpose**: Extract content from presentation files.\n\n**When is it triggered?** When the user needs information from PowerPoint or similar presentation files, or wants to repurpose presentation content.\n\n**What is it supposed to do?** Return slide content including titles, text, and speaker notes in a structured format.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"quarterly_review.pptx\",\n  \"slide_numbers\": \"1-5,8,12\",  // Optional\n  \"max_tokens\": 2000,           // Optional\n  \"continuation_token\": \"...\"    // Optional\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"slides\": [\n      { \n        \"slide_number\": 1, \n        \"title\": \"Q4 2024 Review\",\n        \"content\": \"Quarterly Business Review...\",\n        \"notes\": \"Speaker notes content...\"\n      }\n    ],\n    \"total_slides\": 25,\n    \"token_count\": 1200\n  },\n  \"status\": { \"code\": \"success\" },\n  \"continuation\": { \"has_more\": true, \"token\": \"...\" }\n}\n```\n\n**User Story: \"Create investor pitch from board presentations\"**\n```typescript\n// Step 1: Find recent board decks\nawait search({\n  query: \"board presentation deck 2024\",\n  mode: \"semantic\",\n  scope: \"documents\"\n});\n\n// Step 2: Extract key slides from each\nawait getSlides({\n  document_id: \"board_deck_oct.pptx\",\n  slide_numbers: \"1,5-8,15\"\n});\nawait getSlides({\n  document_id: \"investor_update_q3.pptx\"\n  // Get all slides\n});\n```\n\n**Supported formats**: `.pptx`, `.ppt`, `.odp`\n\n---\n\n### üìÑ `get_pages`\n\n**Purpose**: Extract content from paginated documents.\n\n**When is it triggered?** When the user needs specific pages from PDFs or Word documents, especially after finding relevant sections through search.\n\n**What is it supposed to do?** Return page-by-page content from documents that have clear page boundaries.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"report.pdf\",\n  \"page_range\": \"1-5\",      // Optional\n  \"max_tokens\": 2000,       // Optional\n  \"continuation_token\": \"...\" // Optional\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"pages\": [\n      { \"page_number\": 1, \"content\": \"Annual Report 2024...\" },\n      { \"page_number\": 2, \"content\": \"Table of Contents...\" }\n    ],\n    \"total_pages\": 45,\n    \"token_count\": 1900\n  },\n  \"status\": {\n    \"code\": \"partial_success\",\n    \"message\": \"TOKEN_LIMIT_EXCEEDED_BUT_INCLUDED\"\n  },\n  \"continuation\": {\n    \"has_more\": true,\n    \"token\": \"eyJwYWdlIjozfQ==\"\n  },\n  \"actions\": [\n    {\n      \"id\": \"CONTINUE\",\n      \"description\": \"Get next batch with same token limit\",\n      \"params\": {\"continuation_token\": \"$CONTINUATION_TOKEN\"}\n    },\n    {\n      \"id\": \"INCREASE_LIMIT\",\n      \"description\": \"Retry with higher token limit\",\n      \"params\": {\"max_tokens\": 4000}\n    }\n  ]\n}\n```\n\n**User Story: \"Review legal sections in partner agreements\"**\n```typescript\n// Step 1: Find all partner agreements\nawait search({\n  query: \"partner agreement\",\n  mode: \"semantic\",\n  scope: \"documents\"\n});\n\n// Step 2: Search for legal sections\nawait search({\n  query: \"\\\\b(limitation of liability|indemnification|termination)\\\\b\",\n  mode: \"regex\",\n  scope: \"chunks\"\n});\n\n// Step 3: Get full pages for legal review\nawait getPages({\n  document_id: \"acme_partner_agreement.pdf\",\n  page_range: \"12-18\"\n});\n```\n\n**Supported formats**: `.pdf`, `.docx`, `.doc`, `.rtf`, `.odt`\n\n---\n\n### üß† `get_embedding`\n\n**Purpose**: Return raw vector embedding of a given string (optional for advanced agents).\n\n**When is it triggered?** When an advanced agent needs to compare external text to the knowledge base or implement custom similarity logic.\n\n**What is it supposed to do?** Convert text into the same vector format used internally, enabling custom similarity comparisons.\n\n**Input**:\n\n```json\n{ \"text\": \"Quarterly revenue is up\" }\n```\n\n**Output**:\n\n```json\n{ \"embedding\": [0.31, -0.42, ...] }\n```\n\n**User Story**: \"I have this paragraph from a client email - find all our documents that discuss similar topics\"\n```typescript\n// Step 1: Get embedding for the external text\nconst clientTextEmbedding = await getEmbedding({\n  text: \"We're concerned about supply chain delays affecting Q4 delivery schedules...\"\n});\n\n// Step 2: Agent uses embedding to implement custom similarity search\n// (This requires the agent to have advanced capabilities)\n```\n\n---\n\n### üîÑ `get_status`\n\n**Purpose**: Monitor document ingestion and processing readiness.\n\n**When is it triggered?** When the user adds new documents to the knowledge base or when the agent needs to ensure documents are fully indexed before searching.\n\n**What is it supposed to do?** Return processing status and progress information to prevent searching unindexed documents.\n\n**Input**:\n\n```json\n{ \"document_id\": \"xyz\" }  // Optional\n```\n\n**Output**:\n\n```json\n{\n  \"status\": \"processing\",\n  \"progress\": 74,\n  \"message\": \"DOCUMENT_PROCESSING\"\n}\n```\n\n**User Story: \"Analyze newly added competitive intelligence\"**\n```typescript\n// Step 1: Check processing status\nawait getStatus();\n// Returns: {\"processing\": [\"competitor_analysis.pdf\"], \"completed\": [...]}\n\n// Step 2: Wait for critical document\nawait getStatus({ document_id: \"competitor_analysis.pdf\" });\n// Returns: {\"status\": \"processing\", \"progress\": 78}\n\n// Step 3: Once ready, search for insights\nawait search({\n  query: \"competitor pricing strategy\",\n  mode: \"semantic\",\n  scope: \"chunks\"\n});\n```\n\n---\n\n## üìã Document Format Guide\n\n| Format | Primary Endpoint | Alternate Endpoints |\n|--------|-----------------|-------------------|\n| `.pdf` | `get_pages()` | `get_document_data(format=\"raw\")` |\n| `.docx`, `.doc`, `.rtf`, `.odt` | `get_pages()` | `get_document_data(format=\"raw\")` |\n| `.xlsx`, `.xls`, `.ods` | `get_sheet_data()` | `get_document_data(format=\"metadata\")` |\n| `.csv` | `get_sheet_data()` | `get_document_data(format=\"raw\")` |\n| `.pptx`, `.ppt`, `.odp` | `get_slides()` | `get_document_data(format=\"metadata\")` |\n| `.txt`, `.md`, `.html`, `.xml` | `get_document_data()` | N/A |\n\n---\n\n## üîÑ Standard Response Structure\n\nAll token-limited endpoints follow this structure:\n\n```json\n{\n  \"data\": {\n    // Endpoint-specific data\n    \"token_count\": 1850\n  },\n  \"status\": {\n    \"code\": \"success\" | \"partial_success\" | \"error\",\n    \"message\": \"STATUS_MESSAGE_CODE\"\n  },\n  \"continuation\": {\n    \"has_more\": boolean,\n    \"token\": \"base64_encoded_state\"\n  },\n  \"actions\": [\n    {\n      \"id\": \"CONTINUE\",\n      \"description\": \"Continue with pagination\",\n      \"params\": { \"continuation_token\": \"$CONTINUATION_TOKEN\" }\n    },\n    {\n      \"id\": \"INCREASE_LIMIT\",\n      \"description\": \"Retry with higher token limit\",\n      \"params\": { \"max_tokens\": 4000 }\n    }\n  ]\n}\n```\n\n### Status Message Codes\n\n- `SUCCESS` - Operation completed successfully\n- `TOKEN_LIMIT_EXCEEDED_BUT_INCLUDED` - First item exceeded limit but was included\n- `TOKEN_LIMIT_REACHED` - Stopped at token limit\n- `NO_MORE_DATA` - All data returned\n- `CSV_NO_SHEETS` - CSV files don't have multiple sheets\n\n### Action IDs\n\n- `CONTINUE` - Continue with pagination\n- `INCREASE_LIMIT` - Increase token limit\n- `GET_ALL` - Get all without limits\n- `USE_SPECIFIC_RANGE` - Use specific range instead\n\n---\n\n## üîë Token-Based Pagination\n\nEndpoints with potentially large outputs support token-based pagination:\n\n1. **Default limit**: 2000 tokens (roughly 8000 characters)\n2. **Continuation tokens**: Base64-encoded state for resuming\n3. **Always return at least one item**: Even if it exceeds token limit\n4. **Clear actions**: Guide agents on how to continue\n\nExample continuation token (decoded):\n```json\n{\n  \"document_id\": \"report.pdf\",\n  \"page\": 4,\n  \"type\": \"pdf\"\n}\n```\n\nTypeScript implementation example:\n```typescript\n// Creating a continuation token\nconst state = { document_id: \"report.pdf\", page: 4, type: \"pdf\" };\nconst token = Buffer.from(JSON.stringify(state)).toString('base64url');\n\n// Parsing a continuation token\nconst decoded = JSON.parse(Buffer.from(token, 'base64url').toString('utf8'));\n```\n\n---\n\n## üéØ **Implementation Tasks**\n\n### **Task 1: Remove All Old Endpoints**\n- [x] Delete every endpoint previously registered, including `search_documents`, `get_document_content`, `query_table`, etc.\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 1: Old endpoints removed\"\n```\n\n### **Task 2: Clean Up All Old Tests**\n- [x] Go over all existing test cases and remove or refactor anything dependent on deprecated endpoints\n- [x] Tests for infrastructure (e.g., embedding mechanism, file watching) should remain\n- [x] Focus Phase 1 on clearing technical debt to accelerate new development\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 2: Old tests cleaned up\"\n```\n\n### **Task 3: Create Test Knowledge Base Structure**\n- [x] Create comprehensive test folder structure with various document types\n- [x] Include edge cases: empty files, large files, unicode filenames, corrupted files\n- [x] Populate with test data patterns for financial, legal, and business documents\n\n**Status: ‚úÖ COMPLETED**\n\n‚úÖ **COMPLETED:**\n- Folder structure created (`tests/fixtures/test-knowledge-base/`)\n- Text-based files: `README.md`, `API_Spec.html`, `config.xml`, `notes.txt`\n- CSV file: `Customer_List.csv` with 10 sample records\n- Edge case files: `empty.txt`, `special_chars_Êñá‰ª∂Âêç.txt`, `huge_text.txt` (10MB)\n- Python 3.13.5 installed and PATH configured\n- Office/PDF documents generated with realistic test data:\n  - **Finance**: `Q1_Budget.xlsx`, `Q1_Report.pdf`, `Annual_Report_2024.pdf`, `Q4_Forecast.xlsx` \n  - **Legal**: `Acme_Vendor_Agreement.pdf`, `Supply_Contract_2024.docx`, `Remote_Work_Policy.docx`, `NDA_Template.docx`\n  - **Sales**: `Q4_Board_Deck.pptx`, `Product_Demo.pptx`, `Sales_Pipeline.xlsx`\n  - **Marketing**: `content_calendar.xlsx`, `brand_guidelines.pdf`\n  - **Edge Cases**: `single_page.pdf`, `corrupted.xlsx`\n- All files contain test patterns for emails, financial data, legal terms, dates, and WFH content\n- Generated using faker-file library with ReportLab PDF backend\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 3: Test knowledge base created\"\n```\n\n### **Task 4: Replace With New Tests Based on User Stories**\n- [x] Each new endpoint must be covered by real-world LLM-driven user stories\n- [x] Follow TDD methodology: write failing tests first, then implement endpoints\n\n**Status: ‚úÖ COMPLETED**\n\n‚úÖ **COMPLETED:**\n- Replaced mock endpoint implementation with real `MCPEndpoints` class in tests\n- Created proper dependency injection with `createMockServices()` function\n- Tests now call actual endpoint implementation from `src/interfaces/mcp/endpoints.ts`\n- Successfully established TDD failing state (25 failing tests, 9 passing)\n- All 8 endpoints covered by real-world user stories from PRD\n- Tests follow user scenarios: search, document outline, sheet data, slides, pages, folders, embedding, status\n- Proper error handling expectations and edge cases included\n- Ready for Task 5 implementation\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 4: New user story tests implemented\"\n```\n\n### **Task 5: Implement New Endpoint Specification**\n- [ ] Implement the 8 new streamlined endpoints:\n  - [ ] `search` - Search documents or chunks using semantic similarity or regex patterns\n  - [ ] `get_document_outline` - Get structural overview of any document type without content\n  - [ ] `get_document_data` - Fetch content in raw, structured, or summarized form for general documents\n  - [ ] `list_folders` / `list_documents` - Explore folder structure and contents\n  - [ ] `get_sheet_data` - Extract tabular data from spreadsheet files\n  - [ ] `get_slides` - Extract content from presentation files\n  - [ ] `get_pages` - Extract content from paginated documents\n  - [ ] `get_embedding` - Return raw vector embedding of a given string\n  - [ ] `get_status` - Monitor document ingestion and processing readiness\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 5: New endpoints implemented\"\n```\n\n### **Task 6: Implement Token-Based Pagination**\n- [ ] Implement token-based pagination system\n- [ ] Default limit: 2000 tokens (roughly 8000 characters)\n- [ ] Continuation tokens: Base64-encoded state for resuming\n- [ ] Always return at least one item: Even if it exceeds token limit\n- [ ] Clear actions: Guide agents on how to continue\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 6: Token-based pagination implemented\"\n```\n\n### **Task 7: Implement Metadata Caching**\n- [ ] Document metadata should be extracted and cached immediately after creating/updating/deleting embeddings when files change\n- [ ] Cache should include: file type, size, page/sheet/slide counts, titles, authors, creation dates, and document-specific structure\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 7: Metadata caching implemented\"\n```\n\n### **Task 8: Implement Comprehensive Testing Suite**\n- [ ] Phase 1: Infrastructure Tests (preserve existing)\n- [ ] Phase 2: Individual Endpoint Tests\n- [ ] Phase 3: Integration Tests\n- [ ] Phase 4: Performance Tests\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 8: Comprehensive testing suite implemented\"\n```\n\n---\n\n## ‚úÖ **Testing Plan**\n\n### Test Folder Structure\n\nCreate the following test knowledge base structure:\n\n```\ntest-knowledge-base/\n‚îú‚îÄ‚îÄ Finance/\n‚îÇ   ‚îú‚îÄ‚îÄ 2024/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Q1/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Q1_Budget.xlsx          (multiple sheets, formulas)\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Q1_Report.pdf           (30 pages, bookmarks)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Q4/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Q4_Forecast.xlsx        (large: 5000+ rows)\n‚îÇ   ‚îî‚îÄ‚îÄ Reports/\n‚îÇ       ‚îî‚îÄ‚îÄ Annual_Report_2024.pdf      (100+ pages with bookmarks)\n‚îú‚îÄ‚îÄ Legal/\n‚îÇ   ‚îú‚îÄ‚îÄ Contracts/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Acme_Vendor_Agreement.pdf   (contains emails, dates)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Supply_Contract_2024.docx   (20 pages, termination clause)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ NDA_Template.docx           (standard NDA language)\n‚îÇ   ‚îî‚îÄ‚îÄ Policies/\n‚îÇ       ‚îî‚îÄ‚îÄ Remote_Work_Policy.docx     (contains \"WFH\", \"remote work\")\n‚îú‚îÄ‚îÄ Sales/\n‚îÇ   ‚îú‚îÄ‚îÄ Presentations/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Q4_Board_Deck.pptx          (45 slides with speaker notes)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Product_Demo.pptx           (10 slides, no notes)\n‚îÇ   ‚îî‚îÄ‚îÄ Data/\n‚îÇ       ‚îú‚îÄ‚îÄ Customer_List.csv           (no sheets, 1000 rows)\n‚îÇ       ‚îî‚îÄ‚îÄ Sales_Pipeline.xlsx         (multiple sheets, charts)\n‚îú‚îÄ‚îÄ Marketing/\n‚îÇ   ‚îú‚îÄ‚îÄ content_calendar.xlsx           (12 sheets - one per month)\n‚îÇ   ‚îî‚îÄ‚îÄ brand_guidelines.pdf            (visual-heavy, 15 pages)\n‚îú‚îÄ‚îÄ Engineering/\n‚îÇ   ‚îú‚îÄ‚îÄ README.md                       (markdown with code blocks)\n‚îÇ   ‚îú‚îÄ‚îÄ API_Spec.html                   (structured HTML)\n‚îÇ   ‚îú‚îÄ‚îÄ config.xml                      (XML configuration)\n‚îÇ   ‚îî‚îÄ‚îÄ notes.txt                       (plain text notes)\n‚îî‚îÄ‚îÄ test-edge-cases/\n    ‚îú‚îÄ‚îÄ empty.txt                       (0 bytes)\n    ‚îú‚îÄ‚îÄ huge_text.txt                   (10MB of lorem ipsum)\n    ‚îú‚îÄ‚îÄ single_page.pdf                 (exactly 1 page)\n    ‚îú‚îÄ‚îÄ corrupted.xlsx                  (intentionally malformed)\n    ‚îî‚îÄ‚îÄ special_chars_Êñá‰ª∂Âêç.txt        (unicode filename)\n```\n\n### File Content Requirements\n\n**Financial Files:**\n- Q1_Budget.xlsx: 3 sheets (Summary: 50 rows, Details: 2000 rows, Charts: empty)\n- Include specific values: \"Revenue: $1,234,567\" for testing\n\n**Legal Documents:**\n- Acme_Vendor_Agreement.pdf: Include emails on page 5, \"Limitation of Liability\" on pages 12-14\n- Supply_Contract_2024.docx: Include \"expires December 31, 2024\", email patterns\n\n**Test Data Patterns** - Include across files:\n- Emails: \"john@acme.com\", \"sarah.smith@bigco.com\", \"procurement@supplier.com\"\n- Financial: \"Q1 revenue\", \"quarterly results\", \"$1.2M\"\n- Dates: \"March 31, 2024\", \"Q1 2024\", \"2024-03-31\"\n- Legal: \"force majeure\", \"confidential information\", \"indemnification\"\n\n### Test Implementation (Vitest)\n\n```typescript\nimport { describe, test, expect, beforeAll } from 'vitest';\n\ndescribe('Document Navigation', () => {\n  test('list_folders returns all top-level folders', async () => {\n    const folders = await listFolders();\n    expect(folders).toContain('Finance');\n    expect(folders).toContain('Legal');\n    expect(folders).toContain('Sales');\n  });\n\n  test('list_documents handles pagination', async () => {\n    const page1 = await listDocuments({ \n      folder: 'test-edge-cases',\n      max_tokens: 500 \n    });\n    expect(page1.continuation.has_more).toBe(true);\n  });\n});\n\ndescribe('Search Operations', () => {\n  test('semantic search finds related documents', async () => {\n    const results = await search({\n      query: \"financial performance Q1\",\n      mode: \"semantic\",\n      scope: \"documents\"\n    });\n    \n    const documentIds = results.data.results.map(r => r.document_id);\n    expect(documentIds).toContain('Q1_Budget.xlsx');\n    expect(documentIds).toContain('Q1_Report.pdf');\n  });\n\n  test('regex search finds all email addresses', async () => {\n    const results = await search({\n      query: \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\",\n      mode: \"regex\",\n      scope: \"chunks\"\n    });\n    \n    const foundEmails = results.data.results.map(r => r.preview).flat();\n    expect(foundEmails).toMatch(/contact@acme\\.com/);\n    expect(foundEmails).toMatch(/procurement@supplier\\.com/);\n  });\n\n  test('search returns rich location data', async () => {\n    const results = await search({\n      query: \"limitation of liability\",\n      mode: \"semantic\",\n      scope: \"chunks\"\n    });\n    \n    expect(results.data.results[0].location).toMatchObject({\n      page: expect.any(Number),\n      document_type: 'pdf'\n    });\n  });\n});\n\ndescribe('Content Retrieval', () => {\n  test('get_pages respects token limits', async () => {\n    const result = await getPages({\n      document_id: 'Annual_Report_2024.pdf',\n      max_tokens: 2000\n    });\n    \n    expect(result.data.token_count).toBeLessThanOrEqual(2000);\n    expect(result.continuation.has_more).toBe(true);\n  });\n\n  test('get_sheet_data rejects sheet name for CSV', async () => {\n    await expect(getSheetData({\n      document_id: 'Customer_List.csv',\n      sheet_name: 'Sheet1'\n    })).rejects.toThrow('CSV files don\\'t have multiple sheets');\n  });\n\n  test('continuation tokens work correctly', async () => {\n    const page1 = await getPages({\n      document_id: 'Supply_Contract_2024.docx',\n      max_tokens: 1000\n    });\n    \n    const page2 = await getPages({\n      document_id: 'Supply_Contract_2024.docx',\n      continuation_token: page1.continuation.token\n    });\n    \n    const lastPageNum = page1.data.pages[page1.data.pages.length - 1].page_number;\n    expect(page2.data.pages[0].page_number).toBeGreaterThan(lastPageNum);\n  });\n});\n\ndescribe('Edge Cases', () => {\n  test('handles empty files gracefully', async () => {\n    const result = await getDocumentData({\n      document_id: 'empty.txt',\n      format: 'raw'\n    });\n    expect(result.data.content).toBe('');\n  });\n\n  test('handles unicode filenames', async () => {\n    const result = await getDocumentData({\n      document_id: 'special_chars_Êñá‰ª∂Âêç.txt',\n      format: 'raw'\n    });\n    expect(result.status.code).toBe('success');\n  });\n\n  test('first page exceeds token limit', async () => {\n    const result = await getPages({\n      document_id: 'huge_text.txt',\n      max_tokens: 100  // Very small limit\n    });\n    \n    expect(result.status.code).toBe('partial_success');\n    expect(result.status.message).toBe('TOKEN_LIMIT_EXCEEDED_BUT_INCLUDED');\n    expect(result.actions).toContainEqual(\n      expect.objectContaining({ id: 'INCREASE_LIMIT' })\n    );\n  });\n});\n\ndescribe('User Story: Find Q1 financials and analyze', () => {\n  test('Complete flow from search to data extraction', async () => {\n    // Step 1: Search\n    const searchResults = await search({\n      query: \"Q1 financial results budget\",\n      mode: \"semantic\",\n      scope: \"documents\"\n    });\n    expect(searchResults.data.results.length).toBeGreaterThan(0);\n    \n    // Step 2: Get outline to understand structure\n    const outline = await getDocumentOutline('Q1_Budget.xlsx');\n    expect(outline.sheets).toContainEqual(\n      expect.objectContaining({ name: 'Summary' })\n    );\n    \n    // Step 3: Get specific data\n    const sheetData = await getSheetData({\n      document_id: 'Q1_Budget.xlsx',\n      sheet_name: 'Summary'\n    });\n    \n    const revenueRow = sheetData.data.rows.find(row => \n      row.some(cell => cell.includes('$1,234,567'))\n    );\n    expect(revenueRow).toBeDefined();\n  });\n});\n```\n\n### Test Execution Strategy\n\n1. **Phase 1: Infrastructure Tests** (preserve existing)\n   - Embedding generation and updates\n   - File watching and change detection\n   - Cache operations and metadata storage\n\n2. **Phase 2: Individual Endpoint Tests**\n   - Test each endpoint in isolation\n   - Verify all response fields match specification\n   - Test error conditions and edge cases\n   - Validate token counting and pagination\n\n3. **Phase 3: Integration Tests**\n   - Multi-step user story flows\n   - Token limit scenarios with continuation\n   - Cross-endpoint data consistency\n\n4. **Phase 4: Performance Tests**\n   - Large file handling (10MB+ files)\n   - Concurrent request handling\n   - Cache effectiveness metrics\n   - Response time benchmarks\n\n---\n\n## üîß Implementation Notes\n\n### Metadata Caching\nDocument metadata should be extracted and cached immediately after creating/updating/deleting embeddings when files change. This ensures:\n- `get_document_outline()` returns instantly without re-parsing\n- `get_document_data(format=\"metadata\")` responds without file access\n- Search results can include rich metadata without additional lookups\n\nCache should include: file type, size, page/sheet/slide counts, titles, authors, creation dates, and any document-specific structure (bookmarks, sheet names, etc.).\n\n---\n\n## üìå Conclusion\n\nThis PRD transitions the MCP server from a tool-centric, low-level API to a **user-intent-driven and LLM-friendly interface**, with:\n\n- 8 streamlined endpoints (vs 13 originally)\n- Token-aware pagination preventing context overflow\n- Rich search results enabling precise retrieval\n- Document outlines for efficient exploration\n- Clear action guidance for LLM agents\n\nThe design balances flexibility with clarity, providing focused endpoints that complete user journeys while respecting LLM token constraints.",
    "type": "md",
    "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
    "metadata": {
      "type": "md",
      "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
      "size": 34455,
      "lastModified": "2025-06-18T17:34:35.017Z",
      "lines": 1149,
      "encoding": "utf-8"
    }
  },
  "chunks": [
    {
      "content": "# Product Requirements Document (PRD): MCP Endpoint Redesign for Folder-MCP v2.0\n\n## üß≠ Overview\n\nThis document outlines the second-generation redesign of the MCP (Mesh Control Protocol) endpoints for the `folder-mcp` tool. The tool turns any local folder into an AI-accessible knowledge base by indexing its content and exposing it through a streamlined, LLM-friendly API.\n\n### üîÑ What's Changing and Why\n\nThe original version exposed too many low-level, granular endpoints that required LLMs to orchestrate multi-step workflows inefficiently. The new design consolidates and simplifies the API into a smaller set of intent-driven, semantically meaningful endpoints. The goals are to:\n\n- Reduce planning complexity for LLMs\n- Improve speed by reducing endpoint chatter\n- Align responses with token limits and LLM context boundaries\n- Allow flexible exploration of local documents through structured access\n\n### üìå Migration Requirements\n\n1. **REMOVE ALL OLD ENDPOINTS**: Delete every endpoint previously registered, including `search_documents`, `get_document_content`, `query_table`, etc.\n2. **CLEAN UP ALL OLD TESTS**: Go over all existing test cases and remove or refactor anything dependent on deprecated endpoints.\n   - ‚ö†Ô∏è Tests for infrastructure (e.g., embedding mechanism, file watching) should remain.\n   - ‚úÖ Focus Phase 1 on clearing technical debt to accelerate new development.\n3. **REPLACE WITH NEW TESTS BASED ON USER STORIES**: Each new endpoint must be covered by real-world LLM-driven user stories (see below).\n\n---\n\n## üö® **Safety Framework**\n\n### **Backup Strategy**\n```powershell\n# Create backup branch before starting\ngit checkout -b backup/pre-mcp-endpoint-redesign\ngit add -A\ngit commit -m \"Backup before MCP endpoint redesign implementation\"\n\n# Create implementation branch  \ngit checkout -b feature/mcp-endpoint-redesign\n```\n\n### **Rollback Plan**\n```powershell\n# If major issues arise, return to backup\ngit checkout backup/pre-mcp-endpoint-redesign \ngit checkout -b feature/mcp-endpoint-redesign-retry\n```\n\n### **Validation Commands**\n```powershell\n# Run after each major task completion\nnpm run build        # Must compile without errors\nnpm test             # All tests must pass\ngit status           # Verify clean working state\n```\n\n---\n\n## ‚úÖ New Endpoint Specification\n\nEach endpoint includes:\n\n- Purpose\n- Input/Output spec\n- User story\n- Metadata (optional but recommended)\n\n### üîç `search`\n\n**Purpose**: Search documents or chunks using semantic similarity or regex patterns.\n\n**When is it triggered?** When the user asks to find information, documents, or patterns in their knowledge base.",
      "startPosition": 0,
      "endPosition": 2625,
      "tokenCount": 476,
      "chunkIndex": 0,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": false,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "**When is it triggered?** When the user asks to find information, documents, or patterns in their knowledge base.\n\n**What is it supposed to do?** Return relevant documents/chunks with rich context and location information to enable precise retrieval.\n\n**Input**:\n\n```json\n{\n  \"query\": \"Q1 financial results\",  // Natural language OR regex pattern\n  \"mode\": \"semantic\" | \"regex\",\n  \"scope\": \"documents\" | \"chunks\",\n  \"filters\": { \"folder\": \"optional-folder-name\", \"fileType\": \"pdf\" },\n  \"max_tokens\": 2000,  // Optional, defaults to 2000\n  \"continuation_token\": \"...\"  // Optional, for pagination\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"results\": [\n      {\n        \"document_id\": \"abc\",\n        \"preview\": \"Revenue grew by 15%...\",\n        \"score\": 0.92,\n        \"location\": {\n          \"page\": 23,\n          \"section\": \"Financial Results\",\n          \"sheet\": null,\n          \"slide\": null\n        },\n        \"context\": {\n          \"before\": \"Q2 showed steady growth...\",\n          \"after\": \"This trend is expected...\"\n        },\n        \"metadata\": {\n          \"document_type\": \"pdf\",\n          \"total_pages\": 94\n        }\n      }\n    ],\n    \"token_count\": 1850\n  },\n  \"status\": {\n    \"code\": \"success\",\n    \"message\": \"SEARCH_COMPLETED\"\n  },\n  \"continuation\": {\n    \"has_more\": true,\n    \"token\": \"eyJvZmZzZXQiOjEwfQ==\"\n  }\n}\n```\n\n**User Stories**:\n\n**Story 1: \"Find last month's sales performance and analyze trends\"**\n```typescript\n// Step 1: Search for sales data\nawait search({\n  query: \"sales performance october 2024\",\n  mode: \"semantic\",\n  scope: \"documents\"\n});\n// Returns: [{\"document_id\": \"sales_report_oct.xlsx\", \"preview\": \"Total sales: $1.2M...\", \"location\": {\"sheet\": \"Summary\"}}, \n//          {\"document_id\": \"board_deck_oct.pptx\", \"preview\": \"Sales exceeded target by 15%...\", \"location\": {\"slide\": 8}}]\n\n// Step 2: Get detailed data from spreadsheet\nawait getSheetData({\n  document_id: \"sales_report_oct.xlsx\",\n  sheet_name: \"Summary\"\n});\n\n// Step 3: Get presentation insights\nawait getSlides({\n  document_id: \"board_deck_oct.pptx\",\n  slide_numbers: \"8-12\"\n});\n```\n\n**Story 2: \"Find all vendor contracts and check their expiration dates\"**\n```typescript\n// Step 1: Search for contract patterns\nawait search({\n  query: \"\\\\b(contract|agreement)\\\\b.*\\\\b(vendor|supplier)\\\\b\",\n  mode: \"regex\",\n  scope: \"chunks\"\n});\n// Returns: Multiple hits with exact page locations\n\n// Step 2: For each result, get the full context\nawait getPages({\n  document_id: \"acme_vendor_agreement.pdf\",\n  page_range: \"1,15\"  // First and signature pages\n});\nawait getPages({\n  document_id: \"supplies_contract_2024.docx\"\n  // Get all pages to find dates\n});\n```\n\n**Metadata**:\n\n```json\n{\n  \"description\": \"Unified search with semantic or pattern matching\",\n  \"recommendedUsage\": \"Use mode='semantic' for concepts, mode='regex' for patterns\"\n}\n```\n\n---\n\n### üìÑ `get_document_outline`\n\n**Purpose**: Get structural overview of any document type without content.\n\n**When is it triggered?** When the user needs to understand what's in a document before retrieving specific parts, or when dealing with large documents.\n\n**What is it supposed to do?** Return document structure (pages, sheets, slides, bookmarks) without the actual content, enabling targeted retrieval.",
      "startPosition": 2512,
      "endPosition": 5770,
      "tokenCount": 500,
      "chunkIndex": 1,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "**When is it triggered?** When the user needs to understand what's in a document before retrieving specific parts, or when dealing with large documents.\n\n**What is it supposed to do?** Return document structure (pages, sheets, slides, bookmarks) without the actual content, enabling targeted retrieval.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"report.pdf\"\n}\n```\n\n**Output varies by document type**:\n\n**For PDFs:**\n```json\n{\n  \"type\": \"pdf\",\n  \"total_pages\": 94,\n  \"bookmarks\": [\n    {\"title\": \"Introduction\", \"page\": 1},\n    {\"title\": \"Financial Overview\", \"page\": 23}\n  ],\n  \"file_size\": \"2.4MB\"\n}\n```\n\n**For Excel/Sheets:**\n```json\n{\n  \"type\": \"xlsx\",\n  \"sheets\": [\n    {\"name\": \"Revenue\", \"rows\": 2847, \"columns\": 12},\n    {\"name\": \"Summary\", \"rows\": 4, \"columns\": 8},\n    {\"name\": \"Charts\", \"rows\": 0, \"columns\": 0}\n  ],\n  \"total_rows\": 2851,\n  \"file_size\": \"1.2MB\"\n}\n```\n\n**For PowerPoint:**\n```json\n{\n  \"type\": \"pptx\",\n  \"total_slides\": 45,\n  \"slides\": [\n    {\"number\": 1, \"title\": \"Q4 Business Review\"},\n    {\"number\": 2, \"title\": \"Agenda\"},\n    {\"number\": 3, \"title\": null}\n  ],\n  \"file_size\": \"15.3MB\"\n}\n```\n\n**User Story: \"What's in this 100-page report? I need the financial section\"**\n```typescript\n// Step 1: Get outline to see what's in the document\nawait getDocumentOutline(\"annual_report_2024.pdf\");\n// Returns: {\"type\": \"pdf\", \"total_pages\": 94, \"bookmarks\": [\n//   {\"title\": \"Executive Summary\", \"page\": 1},\n//   {\"title\": \"Financial Overview\", \"page\": 23},\n//   {\"title\": \"Risk Factors\", \"page\": 45}\n// ]}\n\n// Step 2: Extract just the financial section\nawait getPages({\n  document_id: \"annual_report_2024.pdf\",\n  page_range: \"23-44\"  // Financial section based on bookmarks\n});\n```\n\n---\n\n### üìÑ `get_document_data`\n\n**Purpose**: Fetch content in raw, structured, or summarized form for general documents.\n\n**When is it triggered?** When the user needs document content that doesn't fit specialized formats (not paginated, not spreadsheets, not slides).\n\n**What is it supposed to do?** Return text content from simple documents like .txt, .md, .html, .xml files in various formats.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"abc\",\n  \"format\": \"raw\" | \"chunks\" | \"metadata\",\n  \"section\": \"optional-section-id\",\n  \"max_tokens\": 2000,  // Optional\n  \"continuation_token\": \"...\"  // Optional\n}\n```\n\n**Output**: Depends on `format`:\n\n- `raw`: `{ \"content\": \"Full document text...\" }`\n- `chunks`: `[{ \"chunk_id\": \"123\", \"content\": \"...\", \"metadata\": {...} }]`\n- `metadata`: `{ \"title\": \"...\", \"author\": \"...\", \"created\": \"...\", \"pages\": 10 }`\n\n**Implementation Note**: Metadata should be extracted and cached immediately after creating/updating/deleting embeddings when a file changes. This ensures `format=\"metadata\"` responses are instant without re-parsing documents.",
      "startPosition": 5468,
      "endPosition": 8248,
      "tokenCount": 466,
      "chunkIndex": 2,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "This ensures `format=\"metadata\"` responses are instant without re-parsing documents.\n\n**User Story: \"Research company's remote work policy\"**\n```typescript\n// Step 1: Search for remote work policies\nawait search({\n  query: \"remote work policy guidelines WFH\",\n  mode: \"semantic\",\n  scope: \"chunks\"\n});\n\n// Step 2: Get full context for relevant documents\nawait getDocumentData({\n  document_id: \"employee_handbook_2024.pdf\",\n  format: \"chunks\"\n});\n\n// Step 3: Get the full remote work policy document\nawait getDocumentData({\n  document_id: \"remote_work_policy_v3.docx\",\n  format: \"raw\"\n});\n```\n\n**Supported formats**: `.txt`, `.md`, `.html`, `.xml`\n\n---\n\n### üìÅ `list_folders` / `list_documents`\n\n**Purpose**: Explore folder structure and contents.\n\n**When is it triggered?** When the user wants to browse their knowledge base, find documents in specific folders, or understand the organization structure.\n\n**What is it supposed to do?** Return folder hierarchies and document listings to enable navigation through the knowledge base.\n\n**Input**:\n\n- `list_folders()` has no input\n- `list_documents(folder)`:\n\n```json\n{ \n  \"folder\": \"name\",\n  \"max_tokens\": 2000,  // Optional for large folders\n  \"continuation_token\": \"...\"  // Optional\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"documents\": [\n      {\"name\": \"Q1_Report.pdf\", \"document_id\": \"abc\", \"modified\": \"2024-05-02\"}\n    ],\n    \"token_count\": 450\n  },\n  \"status\": { \"code\": \"success\" },\n  \"continuation\": { \"has_more\": false }\n}\n```\n\n**User Story: \"Find all Q4 financial documents by department\"**\n```typescript\n// Step 1: Explore folder structure\nawait listFolders();\n// Returns: [\"Finance\", \"Sales\", \"Marketing\", \"Operations\", ...]\n\n// Step 2: Check Finance folder for Q4 docs\nawait listDocuments({ folder: \"Finance/2024/Q4\" });\n\n// Step 3: Check other departments\nawait listDocuments({ folder: \"Sales/Reports/Q4\" });\n```\n\n---\n\n### üìä `get_sheet_data`\n\n**Purpose**: Extract tabular data from spreadsheet files.\n\n**When is it triggered?** When the user needs data from Excel, CSV, or similar files for analysis, calculations, or data extraction.\n\n**What is it supposed to do?** Return structured tabular data with headers and rows, ready for analysis or processing.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"financials.xlsx\",\n  \"sheet_name\": \"Revenue\",  // Optional, error for CSV if provided\n  \"cell_range\": \"A1:D10\",   // Optional\n  \"max_tokens\": 2000,       // Optional\n  \"continuation_token\": \"...\" // Optional\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"headers\": [\"Month\", \"Revenue\"],\n    \"rows\": [[\"Jan\", \"$1000\"], ...],\n    \"token_count\": 1850\n  },\n  \"status\": {\n    \"code\": \"success\"\n  },\n  \"continuation\": {\n    \"has_more\": true,\n    \"token\": \"eyJyb3ciOjEwMX0=\"\n  },\n  \"actions\": [\n    {\n      \"id\": \"CONTINUE\",\n      \"description\": \"Get next batch of rows\",\n      \"params\": {\"continuation_token\": \"$CONTINUATION_TOKEN\"}\n    }\n  ]\n}\n```\n\n**CSV Note**: Returns error if sheet_name is provided: \"CSV files don't have multiple sheets. Omit sheet_name parameter.\"",
      "startPosition": 8164,
      "endPosition": 11194,
      "tokenCount": 492,
      "chunkIndex": 3,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "Omit sheet_name parameter.\"\n\n**User Story: \"Analyze customer churn across sources\"**\n```typescript\n// Step 1: Search for churn data\nawait search({\n  query: \"customer churn retention analysis\",\n  mode: \"semantic\",\n  scope: \"documents\"\n});\n\n// Step 2: Get main analysis file\nawait getSheetData({\n  document_id: \"churn_analysis_2024.xlsx\",\n  sheet_name: \"Monthly_Churn\"\n});\n\n// Step 3: Get raw CSV data\nawait getSheetData({\n  document_id: \"retention_report.csv\"\n  // No sheet_name for CSV\n});\n```\n\n**Supported formats**: `.xlsx`, `.xls`, `.ods`, `.csv`\n\n---\n\n### üéØ `get_slides`\n\n**Purpose**: Extract content from presentation files.\n\n**When is it triggered?** When the user needs information from PowerPoint or similar presentation files, or wants to repurpose presentation content.\n\n**What is it supposed to do?** Return slide content including titles, text, and speaker notes in a structured format.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"quarterly_review.pptx\",\n  \"slide_numbers\": \"1-5,8,12\",  // Optional\n  \"max_tokens\": 2000,           // Optional\n  \"continuation_token\": \"...\"    // Optional\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"slides\": [\n      { \n        \"slide_number\": 1, \n        \"title\": \"Q4 2024 Review\",\n        \"content\": \"Quarterly Business Review...\",\n        \"notes\": \"Speaker notes content...\"\n      }\n    ],\n    \"total_slides\": 25,\n    \"token_count\": 1200\n  },\n  \"status\": { \"code\": \"success\" },\n  \"continuation\": { \"has_more\": true, \"token\": \"...\" }\n}\n```\n\n**User Story: \"Create investor pitch from board presentations\"**\n```typescript\n// Step 1: Find recent board decks\nawait search({\n  query: \"board presentation deck 2024\",\n  mode: \"semantic\",\n  scope: \"documents\"\n});\n\n// Step 2: Extract key slides from each\nawait getSlides({\n  document_id: \"board_deck_oct.pptx\",\n  slide_numbers: \"1,5-8,15\"\n});\nawait getSlides({\n  document_id: \"investor_update_q3.pptx\"\n  // Get all slides\n});\n```\n\n**Supported formats**: `.pptx`, `.ppt`, `.odp`\n\n---\n\n### üìÑ `get_pages`\n\n**Purpose**: Extract content from paginated documents.\n\n**When is it triggered?** When the user needs specific pages from PDFs or Word documents, especially after finding relevant sections through search.\n\n**What is it supposed to do?** Return page-by-page content from documents that have clear page boundaries.\n\n**Input**:\n\n```json\n{\n  \"document_id\": \"report.pdf\",\n  \"page_range\": \"1-5\",      // Optional\n  \"max_tokens\": 2000,       // Optional\n  \"continuation_token\": \"...\" // Optional\n}\n```\n\n**Output**:\n\n```json\n{\n  \"data\": {\n    \"pages\": [\n      { \"page_number\": 1, \"content\": \"Annual Report 2024...\" },\n      { \"page_number\": 2, \"content\": \"Table of Contents...\" }\n    ],\n    \"total_pages\": 45,\n    \"token_count\": 1900\n  },\n  \"status\": {\n    \"code\": \"partial_success\",\n    \"message\": \"TOKEN_LIMIT_EXCEEDED_BUT_INCLUDED\"\n  },\n  \"continuation\": {\n    \"has_more\": true,\n    \"token\": \"eyJwYWdlIjozfQ==\"\n  },\n  \"actions\": [\n    {\n      \"id\": \"CONTINUE\",\n      \"description\": \"Get next batch with same token limit\",\n      \"params\": {\"continuation_token\": \"$CONTINUATION_TOKEN\"}\n    },\n    {\n      \"id\": \"INCREASE_LIMIT\",\n      \"description\": \"Retry with higher token limit\",\n      \"params\": {\"max_tokens\": 4000}\n    }\n  ]\n}\n```",
      "startPosition": 11167,
      "endPosition": 14388,
      "tokenCount": 496,
      "chunkIndex": 4,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "\n\n**User Story: \"Review legal sections in partner agreements\"**\n```typescript\n// Step 1: Find all partner agreements\nawait search({\n  query: \"partner agreement\",\n  mode: \"semantic\",\n  scope: \"documents\"\n});\n\n// Step 2: Search for legal sections\nawait search({\n  query: \"\\\\b(limitation of liability|indemnification|termination)\\\\b\",\n  mode: \"regex\",\n  scope: \"chunks\"\n});\n\n// Step 3: Get full pages for legal review\nawait getPages({\n  document_id: \"acme_partner_agreement.pdf\",\n  page_range: \"12-18\"\n});\n```\n\n**Supported formats**: `.pdf`, `.docx`, `.doc`, `.rtf`, `.odt`\n\n---\n\n### üß† `get_embedding`\n\n**Purpose**: Return raw vector embedding of a given string (optional for advanced agents).\n\n**When is it triggered?** When an advanced agent needs to compare external text to the knowledge base or implement custom similarity logic.\n\n**What is it supposed to do?** Convert text into the same vector format used internally, enabling custom similarity comparisons.\n\n**Input**:\n\n```json\n{ \"text\": \"Quarterly revenue is up\" }\n```\n\n**Output**:\n\n```json\n{ \"embedding\": [0.31, -0.42, ...] }\n```\n\n**User Story**: \"I have this paragraph from a client email - find all our documents that discuss similar topics\"\n```typescript\n// Step 1: Get embedding for the external text\nconst clientTextEmbedding = await getEmbedding({\n  text: \"We're concerned about supply chain delays affecting Q4 delivery schedules...\"\n});\n\n// Step 2: Agent uses embedding to implement custom similarity search\n// (This requires the agent to have advanced capabilities)\n```\n\n---\n\n### üîÑ `get_status`\n\n**Purpose**: Monitor document ingestion and processing readiness.\n\n**When is it triggered?** When the user adds new documents to the knowledge base or when the agent needs to ensure documents are fully indexed before searching.\n\n**What is it supposed to do?** Return processing status and progress information to prevent searching unindexed documents.\n\n**Input**:\n\n```json\n{ \"document_id\": \"xyz\" }  // Optional\n```\n\n**Output**:\n\n```json\n{\n  \"status\": \"processing\",\n  \"progress\": 74,\n  \"message\": \"DOCUMENT_PROCESSING\"\n}\n```\n\n**User Story: \"Analyze newly added competitive intelligence\"**\n```typescript\n// Step 1: Check processing status\nawait getStatus();\n// Returns: {\"processing\": [\"competitor_analysis.pdf\"], \"completed\": [...]}\n\n// Step 2: Wait for critical document\nawait getStatus({ document_id: \"competitor_analysis.pdf\" });\n// Returns: {\"status\": \"processing\", \"progress\": 78}\n\n// Step 3: Once ready, search for insights\nawait search({\n  query: \"competitor pricing strategy\",\n  mode: \"semantic\",\n  scope: \"chunks\"\n});\n```\n\n---\n\n## üìã Document Format Guide",
      "startPosition": 14388,
      "endPosition": 17015,
      "tokenCount": 459,
      "chunkIndex": 5,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "\n\n| Format | Primary Endpoint | Alternate Endpoints |\n|--------|-----------------|-------------------|\n| `.pdf` | `get_pages()` | `get_document_data(format=\"raw\")` |\n| `.docx`, `.doc`, `.rtf`, `.odt` | `get_pages()` | `get_document_data(format=\"raw\")` |\n| `.xlsx`, `.xls`, `.ods` | `get_sheet_data()` | `get_document_data(format=\"metadata\")` |\n| `.csv` | `get_sheet_data()` | `get_document_data(format=\"raw\")` |\n| `.pptx`, `.ppt`, `.odp` | `get_slides()` | `get_document_data(format=\"metadata\")` |\n| `.txt`, `.md`, `.html`, `.xml` | `get_document_data()` | N/A |\n\n---\n\n## üîÑ Standard Response Structure\n\nAll token-limited endpoints follow this structure:\n\n```json\n{\n  \"data\": {\n    // Endpoint-specific data\n    \"token_count\": 1850\n  },\n  \"status\": {\n    \"code\": \"success\" | \"partial_success\" | \"error\",\n    \"message\": \"STATUS_MESSAGE_CODE\"\n  },\n  \"continuation\": {\n    \"has_more\": boolean,\n    \"token\": \"base64_encoded_state\"\n  },\n  \"actions\": [\n    {\n      \"id\": \"CONTINUE\",\n      \"description\": \"Continue with pagination\",\n      \"params\": { \"continuation_token\": \"$CONTINUATION_TOKEN\" }\n    },\n    {\n      \"id\": \"INCREASE_LIMIT\",\n      \"description\": \"Retry with higher token limit\",\n      \"params\": { \"max_tokens\": 4000 }\n    }\n  ]\n}\n```\n\n### Status Message Codes\n\n- `SUCCESS` - Operation completed successfully\n- `TOKEN_LIMIT_EXCEEDED_BUT_INCLUDED` - First item exceeded limit but was included\n- `TOKEN_LIMIT_REACHED` - Stopped at token limit\n- `NO_MORE_DATA` - All data returned\n- `CSV_NO_SHEETS` - CSV files don't have multiple sheets\n\n### Action IDs\n\n- `CONTINUE` - Continue with pagination\n- `INCREASE_LIMIT` - Increase token limit\n- `GET_ALL` - Get all without limits\n- `USE_SPECIFIC_RANGE` - Use specific range instead\n\n---\n\n## üîë Token-Based Pagination\n\nEndpoints with potentially large outputs support token-based pagination:\n\n1. **Default limit**: 2000 tokens (roughly 8000 characters)\n2. **Continuation tokens**: Base64-encoded state for resuming\n3. **Always return at least one item**: Even if it exceeds token limit\n4. **Clear actions**: Guide agents on how to continue\n\nExample continuation token (decoded):\n```json\n{\n  \"document_id\": \"report.pdf\",\n  \"page\": 4,\n  \"type\": \"pdf\"\n}\n```\n\nTypeScript implementation example:\n```typescript\n// Creating a continuation token\nconst state = { document_id: \"report.pdf\", page: 4, type: \"pdf\" };\nconst token = Buffer.from(JSON.stringify(state)).toString('base64url');\n\n// Parsing a continuation token\nconst decoded = JSON.parse(Buffer.from(token, 'base64url').toString('utf8'));\n```\n\n---\n\n## üéØ **Implementation Tasks**\n\n### **Task 1: Remove All Old Endpoints**\n- [x] Delete every endpoint previously registered, including `search_documents`, `get_document_content`, `query_table`, etc.\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 1: Old endpoints removed\"\n```",
      "startPosition": 17015,
      "endPosition": 19895,
      "tokenCount": 458,
      "chunkIndex": 6,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 1: Old endpoints removed\"\n```\n\n### **Task 2: Clean Up All Old Tests**\n- [x] Go over all existing test cases and remove or refactor anything dependent on deprecated endpoints\n- [x] Tests for infrastructure (e.g., embedding mechanism, file watching) should remain\n- [x] Focus Phase 1 on clearing technical debt to accelerate new development\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 2: Old tests cleaned up\"\n```\n\n### **Task 3: Create Test Knowledge Base Structure**\n- [x] Create comprehensive test folder structure with various document types\n- [x] Include edge cases: empty files, large files, unicode filenames, corrupted files\n- [x] Populate with test data patterns for financial, legal, and business documents\n\n**Status: ‚úÖ COMPLETED**\n\n‚úÖ **COMPLETED:**\n- Folder structure created (`tests/fixtures/test-knowledge-base/`)\n- Text-based files: `README.md`, `API_Spec.html`, `config.xml`, `notes.txt`\n- CSV file: `Customer_List.csv` with 10 sample records\n- Edge case files: `empty.txt`, `special_chars_Êñá‰ª∂Âêç.txt`, `huge_text.txt` (10MB)\n- Python 3.13.5 installed and PATH configured\n- Office/PDF documents generated with realistic test data:\n  - **Finance**: `Q1_Budget.xlsx`, `Q1_Report.pdf`, `Annual_Report_2024.pdf`, `Q4_Forecast.xlsx` \n  - **Legal**: `Acme_Vendor_Agreement.pdf`, `Supply_Contract_2024.docx`, `Remote_Work_Policy.docx`, `NDA_Template.docx`\n  - **Sales**: `Q4_Board_Deck.pptx`, `Product_Demo.pptx`, `Sales_Pipeline.xlsx`\n  - **Marketing**: `content_calendar.xlsx`, `brand_guidelines.pdf`\n  - **Edge Cases**: `single_page.pdf`, `corrupted.xlsx`\n- All files contain test patterns for emails, financial data, legal terms, dates, and WFH content\n- Generated using faker-file library with ReportLab PDF backend\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 3: Test knowledge base created\"\n```\n\n### **Task 4: Replace With New Tests Based on User Stories**\n- [x] Each new endpoint must be covered by real-world LLM-driven user stories\n- [x] Follow TDD methodology: write failing tests first, then implement endpoints\n\n**Status: ‚úÖ COMPLETED**\n\n‚úÖ **COMPLETED:**\n- Replaced mock endpoint implementation with real `MCPEndpoints` class in tests\n- Created proper dependency injection with `createMockServices()` function\n- Tests now call actual endpoint implementation from `src/interfaces/mcp/endpoints.ts`\n- Successfully established TDD failing state (25 failing tests, 9 passing)\n- All 8 endpoints covered by real-world user stories from PRD\n- Tests follow user scenarios: search, document outline, sheet data, slides, pages, folders, embedding, status\n- Proper error handling expectations and edge cases included\n- Ready for Task 5 implementation",
      "startPosition": 19759,
      "endPosition": 22646,
      "tokenCount": 502,
      "chunkIndex": 7,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 4: New user story tests implemented\"\n```\n\n### **Task 5: Implement New Endpoint Specification**\n- [ ] Implement the 8 new streamlined endpoints:\n  - [ ] `search` - Search documents or chunks using semantic similarity or regex patterns\n  - [ ] `get_document_outline` - Get structural overview of any document type without content\n  - [ ] `get_document_data` - Fetch content in raw, structured, or summarized form for general documents\n  - [ ] `list_folders` / `list_documents` - Explore folder structure and contents\n  - [ ] `get_sheet_data` - Extract tabular data from spreadsheet files\n  - [ ] `get_slides` - Extract content from presentation files\n  - [ ] `get_pages` - Extract content from paginated documents\n  - [ ] `get_embedding` - Return raw vector embedding of a given string\n  - [ ] `get_status` - Monitor document ingestion and processing readiness\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 5: New endpoints implemented\"\n```\n\n### **Task 6: Implement Token-Based Pagination**\n- [ ] Implement token-based pagination system\n- [ ] Default limit: 2000 tokens (roughly 8000 characters)\n- [ ] Continuation tokens: Base64-encoded state for resuming\n- [ ] Always return at least one item: Even if it exceeds token limit\n- [ ] Clear actions: Guide agents on how to continue\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 6: Token-based pagination implemented\"\n```\n\n### **Task 7: Implement Metadata Caching**\n- [ ] Document metadata should be extracted and cached immediately after creating/updating/deleting embeddings when files change\n- [ ] Cache should include: file type, size, page/sheet/slide counts, titles, authors, creation dates, and document-specific structure\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 7: Metadata caching implemented\"\n```\n\n### **Task 8: Implement Comprehensive Testing Suite**\n- [ ] Phase 1: Infrastructure Tests (preserve existing)\n- [ ] Phase 2: Individual Endpoint Tests\n- [ ] Phase 3: Integration Tests\n- [ ] Phase 4: Performance Tests\n\n**Validation After Completion**:\n```powershell\nnpm run build && npm test\ngit add -A && git commit -m \"Task 8: Comprehensive testing suite implemented\"\n```\n\n---",
      "startPosition": 22646,
      "endPosition": 25077,
      "tokenCount": 500,
      "chunkIndex": 8,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "\n\n## ‚úÖ **Testing Plan**\n\n### Test Folder Structure\n\nCreate the following test knowledge base structure:\n\n```\ntest-knowledge-base/\n‚îú‚îÄ‚îÄ Finance/\n‚îÇ   ‚îú‚îÄ‚îÄ 2024/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Q1/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Q1_Budget.xlsx          (multiple sheets, formulas)\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Q1_Report.pdf           (30 pages, bookmarks)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Q4/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Q4_Forecast.xlsx        (large: 5000+ rows)\n‚îÇ   ‚îî‚îÄ‚îÄ Reports/\n‚îÇ       ‚îî‚îÄ‚îÄ Annual_Report_2024.pdf      (100+ pages with bookmarks)\n‚îú‚îÄ‚îÄ Legal/\n‚îÇ   ‚îú‚îÄ‚îÄ Contracts/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Acme_Vendor_Agreement.pdf   (contains emails, dates)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Supply_Contract_2024.docx   (20 pages, termination clause)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ NDA_Template.docx           (standard NDA language)\n‚îÇ   ‚îî‚îÄ‚îÄ Policies/\n‚îÇ       ‚îî‚îÄ‚îÄ Remote_Work_Policy.docx     (contains \"WFH\", \"remote work\")\n‚îú‚îÄ‚îÄ Sales/\n‚îÇ   ‚îú‚îÄ‚îÄ Presentations/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Q4_Board_Deck.pptx          (45 slides with speaker notes)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Product_Demo.pptx           (10 slides, no notes)\n‚îÇ   ‚îî‚îÄ‚îÄ Data/\n‚îÇ       ‚îú‚îÄ‚îÄ Customer_List.csv           (no sheets, 1000 rows)\n‚îÇ       ‚îî‚îÄ‚îÄ Sales_Pipeline.xlsx         (multiple sheets, charts)\n‚îú‚îÄ‚îÄ Marketing/\n‚îÇ   ‚îú‚îÄ‚îÄ content_calendar.xlsx           (12 sheets - one per month)\n‚îÇ   ‚îî‚îÄ‚îÄ brand_guidelines.pdf            (visual-heavy, 15 pages)\n‚îú‚îÄ‚îÄ Engineering/\n‚îÇ   ‚îú‚îÄ‚îÄ README.md                       (markdown with code blocks)\n‚îÇ   ‚îú‚îÄ‚îÄ API_Spec.html                   (structured HTML)\n‚îÇ   ‚îú‚îÄ‚îÄ config.xml                      (XML configuration)\n‚îÇ   ‚îî‚îÄ‚îÄ notes.txt                       (plain text notes)\n‚îî‚îÄ‚îÄ test-edge-cases/\n    ‚îú‚îÄ‚îÄ empty.txt                       (0 bytes)\n    ‚îú‚îÄ‚îÄ huge_text.txt                   (10MB of lorem ipsum)\n    ‚îú‚îÄ‚îÄ single_page.pdf                 (exactly 1 page)\n    ‚îú‚îÄ‚îÄ corrupted.xlsx                  (intentionally malformed)\n    ‚îî‚îÄ‚îÄ special_chars_Êñá‰ª∂Âêç.txt        (unicode filename)\n```\n\n### File Content Requirements\n\n**Financial Files:**\n- Q1_Budget.xlsx: 3 sheets (Summary: 50 rows, Details: 2000 rows, Charts: empty)\n- Include specific values: \"Revenue: $1,234,567\" for testing\n\n**Legal Documents:**\n- Acme_Vendor_Agreement.pdf: Include emails on page 5, \"Limitation of Liability\" on pages 12-14\n- Supply_Contract_2024.docx: Include \"expires December 31, 2024\", email patterns\n\n**Test Data Patterns** - Include across files:\n- Emails: \"john@acme.com\", \"sarah.smith@bigco.com\", \"procurement@supplier.com\"\n- Financial: \"Q1 revenue\", \"quarterly results\", \"$1.2M\"\n- Dates: \"March 31, 2024\", \"Q1 2024\", \"2024-03-31\"\n- Legal: \"force majeure\", \"confidential information\", \"indemnification\"\n\n### Test Implementation (Vitest)\n\n```typescript\nimport { describe, test, expect, beforeAll } from 'vitest';\n\ndescribe('Document Navigation', () => {\n  test('list_folders returns all top-level folders', async () => {\n    const folders = await listFolders();\n    expect(folders).toContain('Finance');\n    expect(folders).toContain('Legal');\n    expect(folders).toContain('Sales');\n  });\n\ntest('list_documents handles pagination', async () => {\n    const page1 = await listDocuments({ \n      folder: 'test-edge-cases',\n      max_tokens: 500 \n    });\n    expect(page1.continuation.has_more).toBe(true);\n  });\n});\n\ndescribe('Search Operations', () => {\n  test('semantic search finds related documents', async () => {\n    const results = await search({\n      query: \"financial performance Q1\",\n      mode: \"semantic\",\n      scope: \"documents\"\n    });\n\nconst documentIds = results.data.results.map(r => r.document_id);\n    expect(documentIds).toContain('Q1_Budget.xlsx');\n    expect(documentIds).toContain('Q1_Report.pdf');\n  });",
      "startPosition": 25077,
      "endPosition": 28620,
      "tokenCount": 500,
      "chunkIndex": 9,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "\n\ntest('regex search finds all email addresses', async () => {\n    const results = await search({\n      query: \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\",\n      mode: \"regex\",\n      scope: \"chunks\"\n    });\n\nconst foundEmails = results.data.results.map(r => r.preview).flat();\n    expect(foundEmails).toMatch(/contact@acme\\.com/);\n    expect(foundEmails).toMatch(/procurement@supplier\\.com/);\n  });\n\ntest('search returns rich location data', async () => {\n    const results = await search({\n      query: \"limitation of liability\",\n      mode: \"semantic\",\n      scope: \"chunks\"\n    });\n\nexpect(results.data.results[0].location).toMatchObject({\n      page: expect.any(Number),\n      document_type: 'pdf'\n    });\n  });\n});\n\ndescribe('Content Retrieval', () => {\n  test('get_pages respects token limits', async () => {\n    const result = await getPages({\n      document_id: 'Annual_Report_2024.pdf',\n      max_tokens: 2000\n    });\n\nexpect(result.data.token_count).toBeLessThanOrEqual(2000);\n    expect(result.continuation.has_more).toBe(true);\n  });\n\ntest('get_sheet_data rejects sheet name for CSV', async () => {\n    await expect(getSheetData({\n      document_id: 'Customer_List.csv',\n      sheet_name: 'Sheet1'\n    })).rejects.toThrow('CSV files don\\'t have multiple sheets');\n  });\n\ntest('continuation tokens work correctly', async () => {\n    const page1 = await getPages({\n      document_id: 'Supply_Contract_2024.docx',\n      max_tokens: 1000\n    });\n\nconst page2 = await getPages({\n      document_id: 'Supply_Contract_2024.docx',\n      continuation_token: page1.continuation.token\n    });\n\nconst lastPageNum = page1.data.pages[page1.data.pages.length - 1].page_number;\n    expect(page2.data.pages[0].page_number).toBeGreaterThan(lastPageNum);\n  });\n});\n\ndescribe('Edge Cases', () => {\n  test('handles empty files gracefully', async () => {\n    const result = await getDocumentData({\n      document_id: 'empty.txt',\n      format: 'raw'\n    });\n    expect(result.data.content).toBe('');\n  });\n\ntest('handles unicode filenames', async () => {\n    const result = await getDocumentData({\n      document_id: 'special_chars_Êñá‰ª∂Âêç.txt',\n      format: 'raw'\n    });\n    expect(result.status.code).toBe('success');\n  });\n\ntest('first page exceeds token limit', async () => {\n    const result = await getPages({\n      document_id: 'huge_text.txt',\n      max_tokens: 100  // Very small limit\n    });\n\nexpect(result.status.code).toBe('partial_success');\n    expect(result.status.message).toBe('TOKEN_LIMIT_EXCEEDED_BUT_INCLUDED');\n    expect(result.actions).toContainEqual(\n      expect.objectContaining({ id: 'INCREASE_LIMIT' })\n    );\n  });\n});\n\ndescribe('User Story: Find Q1 financials and analyze', () => {\n  test('Complete flow from search to data extraction', async () => {\n    // Step 1: Search\n    const searchResults = await search({\n      query: \"Q1 financial results budget\",\n      mode: \"semantic\",\n      scope: \"documents\"\n    });\n    expect(searchResults.data.results.length).toBeGreaterThan(0);\n\n// Step 2: Get outline to understand structure\n    const outline = await getDocumentOutline('Q1_Budget.xlsx');\n    expect(outline.sheets).toContainEqual(\n      expect.objectContaining({ name: 'Summary' })\n    );\n\n// Step 3: Get specific data\n    const sheetData = await getSheetData({\n      document_id: 'Q1_Budget.xlsx',\n      sheet_name: 'Summary'\n    });\n\nconst revenueRow = sheetData.data.rows.find(row => \n      row.some(cell => cell.includes('$1,234,567'))\n    );\n    expect(revenueRow).toBeDefined();\n  });\n});\n```\n\n### Test Execution Strategy\n\n1. **Phase 1: Infrastructure Tests** (preserve existing)\n   - Embedding generation and updates\n   - File watching and change detection\n   - Cache operations and metadata storage\n\n2. **Phase 2: Individual Endpoint Tests**\n   - Test each endpoint in isolation\n   - Verify all response fields match specification\n   - Test error conditions and edge cases\n   - Validate token counting and pagination",
      "startPosition": 28620,
      "endPosition": 32573,
      "tokenCount": 487,
      "chunkIndex": 10,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    },
    {
      "content": "**Phase 1: Infrastructure Tests** (preserve existing)\n   - Embedding generation and updates\n   - File watching and change detection\n   - Cache operations and metadata storage\n\n2. **Phase 2: Individual Endpoint Tests**\n   - Test each endpoint in isolation\n   - Verify all response fields match specification\n   - Test error conditions and edge cases\n   - Validate token counting and pagination\n\n3. **Phase 3: Integration Tests**\n   - Multi-step user story flows\n   - Token limit scenarios with continuation\n   - Cross-endpoint data consistency\n\n4. **Phase 4: Performance Tests**\n   - Large file handling (10MB+ files)\n   - Concurrent request handling\n   - Cache effectiveness metrics\n   - Response time benchmarks\n\n---\n\n## üîß Implementation Notes\n\n### Metadata Caching\nDocument metadata should be extracted and cached immediately after creating/updating/deleting embeddings when files change. This ensures:\n- `get_document_outline()` returns instantly without re-parsing\n- `get_document_data(format=\"metadata\")` responds without file access\n- Search results can include rich metadata without additional lookups\n\nCache should include: file type, size, page/sheet/slide counts, titles, authors, creation dates, and any document-specific structure (bookmarks, sheet names, etc.).\n\n---\n\n## üìå Conclusion\n\nThis PRD transitions the MCP server from a tool-centric, low-level API to a **user-intent-driven and LLM-friendly interface**, with:\n\n- 8 streamlined endpoints (vs 13 originally)\n- Token-aware pagination preventing context overflow\n- Rich search results enabling precise retrieval\n- Document outlines for efficient exploration\n- Clear action guidance for LLM agents\n\nThe design balances flexibility with clarity, providing focused endpoints that complete user journeys while respecting LLM token constraints.",
      "startPosition": 32181,
      "endPosition": 33989,
      "tokenCount": 309,
      "chunkIndex": 11,
      "metadata": {
        "sourceFile": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
        "sourceType": "md",
        "totalChunks": 12,
        "hasOverlap": true,
        "originalMetadata": {
          "type": "md",
          "originalPath": "docs\\development-plan\\1.mcp-endpoint-redesign.md",
          "size": 34455,
          "lastModified": "2025-06-18T17:34:35.017Z",
          "lines": 1149,
          "encoding": "utf-8"
        }
      }
    }
  ],
  "processedAt": "2025-06-18T20:41:34.861Z"
}