# Production-Ready Indexing: Comprehensive Test & Fix Plan

**Goal**: Make our indexing system production-grade reliable, not just pass tests. This plan addresses all potential failure scenarios and edge cases that could break our indexing system in production.

## üö® Critical Production Scenarios We Must Handle

### 1. File System Volatility (Most Common in Production)
- [ ] **During Scanning**: Files added/removed/modified while scanning
- [ ] **During Indexing**: Files added/removed/modified while indexing  
- [ ] **Folder Deletion**: Entire monitored folder deleted while daemon running
- [ ] **Folder Rename**: Monitored folder renamed (appears as delete + create)
- [ ] **Network Drives**: Mounted drives that disconnect/reconnect
- [ ] **Symlinks**: Following/not following, broken symlinks, circular references

### 2. Permission & Access Failures
- [ ] **Read-Only Folder**: Can't create `.folder-mcp` directory
- [ ] **No Read Permission**: Folder exists but can't scan files
- [ ] **Mixed Permissions**: Some files readable, others not (partial indexing)
- [ ] **Permission Changes**: Permissions changed while monitoring
- [ ] **Disk Full**: Can't write embeddings database
- [ ] **File Locks**: Files locked by other processes during indexing

### 3. Daemon Lifecycle Resilience
- [ ] **Crash Recovery**: Daemon crashes mid-indexing, must resume correctly
- [ ] **Graceful Restart**: Save state, restart, continue where left off
- [ ] **Orphaned Processes**: Clean up zombie embedding processes
- [ ] **Multiple Instances**: Prevent duplicate daemons
- [ ] **State Persistence**: Remember folder states across restarts
- [ ] **Configuration Changes**: Handle config updates without data loss

### 4. Concurrent Operation Chaos
- [ ] **Rapid Add/Remove**: Same folder added/removed/added quickly
- [ ] **Duplicate Adds**: Same folder added twice simultaneously
- [ ] **Parallel Indexing**: 10+ folders indexing at once
- [ ] **State Race Conditions**: Multiple state transitions at once
- [ ] **WebSocket Floods**: Multiple clients sending conflicting commands

### 5. Resource Exhaustion
- [ ] **Memory**: Indexing 100,000+ files
- [ ] **CPU**: Throttling under load
- [ ] **File Descriptors**: Too many open files
- [ ] **Inotify Limits**: Linux file watcher limits
- [ ] **Disk I/O**: Slow disks causing timeouts
- [ ] **Network**: Embedding service timeouts

### 6. Database Integrity
- [ ] **Corruption**: Detect and rebuild corrupted databases
- [ ] **Incomplete Writes**: Handle power loss during write
- [ ] **Version Mismatch**: Migrate old database schemas
- [ ] **Lock Contention**: Multiple processes accessing same DB
- [ ] **Backup/Restore**: Ability to recover from backups

### 7. Error Recovery & Resilience
- [ ] **Transient Failures**: Retry with exponential backoff
- [ ] **Partial Success**: Some files indexed, others failed
- [ ] **Dead Letter Queue**: Track permanently failed files
- [ ] **Circuit Breaker**: Stop trying after N failures
- [ ] **Self-Healing**: Automatic recovery from known issues

## üìã Manual Test Scenarios (TMOAT Style)

### Test Suite A: File System Chaos
- [x] **Test 1: Files changing during indexing** ‚úÖ COMPLETED
  1. Add folder with 50 files
  2. Wait for indexing to start
  3. Add 10 new files
  4. Delete 5 files
  5. Modify 5 files
  ‚Üí Verify: System detects changes, re-scans, completes successfully
  ‚Üí **RESULT**: ‚úÖ File changes detected within 2-5 seconds, proper re-indexing flow implemented

- [x] **Test 2: Folder deletion while active** ‚úÖ COMPLETED
  1. Add folder and wait for active state
  2. Delete folder from disk (rm -rf)
  ‚Üí Verify: Error state in FMDM, graceful handling, no crash
  ‚Üí **RESULT**: ‚úÖ Deleted folders removed from FMDM within 30 seconds, no ghost folders

- [ ] **Test 3: Permission denial** ‚è∏Ô∏è DEFERRED
  1. Create folder with files
  2. chmod 000 on some files
  3. Add folder to daemon
  ‚Üí Verify: Partial indexing succeeds, errors logged for inaccessible files
  ‚Üí **STATUS**: Lower priority - core file system chaos issues resolved

### Test Suite B: Daemon Resilience
- [x] **Test 4: Crash recovery** ‚úÖ **FULLY COMPLETED (2025-08-09)**
  1. Add 3 folders, start indexing
  2. Kill -9 daemon process
  3. Restart daemon
  ‚Üí **RESULT**: Database files preserved, daemon restarts cleanly, folders need re-adding but detect existing work
  ‚Üí **Automated Test Created**: `tests/integration/daemon-crash-recovery.test.ts`
  ‚Üí **FIXED (2025-08-09)**: Resolved test failures caused by configuration contamination
    - **Problem**: Tests failing with "Daemon exited unexpectedly" and "FMDM update timeout after 30000ms"
    - **Root Cause**: Daemon startup attempting to restore folders from stale configuration files
    - **Solution**: Implemented isolated configuration directories per test run using `FOLDER_MCP_USER_CONFIG_DIR`
    - **Test Logic**: Updated tests to re-add folders after restart to verify database persistence in isolated environment
    - **Result**: Both crash recovery tests now pass reliably in ~21 seconds

- [x] **Test 5: State persistence** ‚úÖ **FIXED (2025-01-08)**
  1. Add folders in various states (scanning, indexing, active)
  2. Stop daemon gracefully
  3. Start daemon
  ‚Üí **RESULT**: Fixed critical bugs preventing embeddings persistence
  ‚Üí **Bug #1**: Database initialization was using wrong method - Fixed by using `loadIndex()` instead of private property access
  ‚Üí **Bug #2**: Fingerprints stored as timestamps instead of file hashes - Fixed by computing actual MD5 hash
  ‚Üí **Root Cause**: Fingerprints were `fingerprint_${Date.now()}_${i}` instead of actual file hashes
  ‚Üí **Fix Applied**: 
    - Use `loadIndex()` method to properly initialize database without wiping
    - Store actual MD5 file hash as fingerprint instead of timestamp
    - Compare file hashes correctly for change detection
  ‚Üí **NOTE**: Existing databases need to be rebuilt for fix to take effect (one-time migration)

### Test Suite C: Error Conditions
- [ ] **Test 6: Read-only folder**
  1. Create folder with content
  2. Mount as read-only or chmod 555
  3. Add to daemon
  ‚Üí Verify: Error reported clearly, no crash, folder marked as error in FMDM

- [ ] **Test 7: Disk full**
  1. Fill disk to 95%
  2. Add large folder for indexing
  ‚Üí Verify: Graceful failure, clear error message, cleanup attempted

- [ ] **Test 8: Database corruption**
  1. Add folder, let it index
  2. Corrupt embeddings.db (truncate, bad data)
  3. Restart daemon
  ‚Üí Verify: Corruption detected, database rebuilt, user notified

### Test Suite D: Concurrency Stress
- [ ] **Test 9: Parallel operations**
  1. Add 10 folders simultaneously
  2. Remove 5 while others are indexing
  3. Add 5 more
  ‚Üí Verify: No deadlocks, correct final state, all operations complete

- [ ] **Test 10: Rapid state changes**
  1. Add folder
  2. Immediately remove it (before scanning completes)
  3. Add it again
  ‚Üí Verify: Clean state, no orphaned processes, correct final state

## üîß Implementation Fixes Required

### Priority 1: File Monitoring During All States
- [x] **Problem**: File changes during scanning/indexing are missed ‚úÖ COMPLETED
- [x] **Fix**: 
  - ‚úÖ Start file watcher immediately when folder is added
  - ‚úÖ Maintain watcher through all state transitions
  - ‚úÖ Connected change detection to folder state transitions
  - ‚úÖ Implemented `active` ‚Üí `scanning` state transition support

### Priority 2: Daemon State Persistence
- [x] **Problem**: Daemon doesn't restore folder states on restart ‚úÖ COMPLETED
- [x] **Additional fixes**: Fixed embeddings persistence across restarts
  - Database initialization now preserves existing data
  - Fingerprints use MD5 file hashes instead of timestamps  
  - Schema version tracking triggers rebuild on changes
  - Progress tracking shows accurate percentages
  - ‚ö†Ô∏è Partial fix for re-indexing: Filtered unsupported file types
  - ‚ö†Ô∏è Still re-indexes 3 problematic files (empty/corrupted) on each restart
- [x] **Fix**:
  - ‚úÖ Implemented `startAll()` in MonitoredFoldersOrchestrator
  - ‚úÖ Added automatic configuration persistence for folders
  - ‚úÖ Restore folders from configuration on daemon startup
  - ‚úÖ Handle missing/deleted folders during restoration
  - ‚úÖ Fixed database wipe bug - created `addEmbeddings()` for incremental storage
  - ‚úÖ Fixed init bug - proper database connection without wiping data
  - ‚úÖ Embeddings now persist across daemon restarts (29 docs, 6600 chunks preserved)

### Priority 3: Comprehensive Error Handling
- [x] **Problem**: Errors don't update FMDM, no recovery strategy ‚úÖ COMPLETED
- [x] **Fix**:
  - ‚úÖ Added error states to FMDM for folder creation failures
  - ‚úÖ Enhanced error folder tracking in orchestrator
  - ‚úÖ Added folder existence validation with cleanup
  - ‚úÖ Implemented comprehensive TUI error display system
  - ‚úÖ Added proper error status coloring and validation message display
  - ‚úÖ Added terminal resize stability for error folder displays
  - ‚è∏Ô∏è Retry logic and advanced recovery strategies (deferred - comprehensive error display implemented)

### Priority 4: Centralized Extension Management ‚úÖ **COMPLETED (2025-08-09)**
- [x] **Problem**: Supported extensions hardcoded in 17+ files with inconsistencies
- [x] **Root Cause**: No single source of truth for supported file types
- [x] **Fix**:
  - ‚úÖ Created central `src/domain/files/supported-extensions.ts` file
  - ‚úÖ Added comprehensive unit test `tests/unit/supported-extensions.test.ts`
  - ‚úÖ Updated all 15+ files to use `getSupportedExtensions()` import
  - ‚úÖ Added `isDocumentExtension()` utility for extension validation
  - ‚úÖ Centralized extension logic across application, domain, and infrastructure layers

### Priority 5: File State Tracking System ‚úÖ **COMPLETED (2025-08-09)**
- [x] **Problem**: Re-indexing issues due to lack of persistent state tracking
- [x] **Root Cause**: "Dumb change detection" instead of intelligent processing decisions
- [x] **Architectural Flaw Identified**: Files were re-processed repeatedly without remembering processing outcomes
- [x] **Fix**:
  - ‚úÖ Designed and implemented `file_states` table with comprehensive state machine
  - ‚úÖ Created FileStateManager domain service for intelligent processing decisions
  - ‚úÖ Created SqliteFileStateStorage infrastructure for persistent state management
  - ‚úÖ Updated scanning logic to use file state decisions instead of fingerprint comparison
  - ‚úÖ Track states: pending, processing, indexed, failed, skipped, corrupted
  - ‚úÖ Content-based hashing (MD5) with file path, content, size, mtime for change detection
  - ‚úÖ Intelligent decisions: shouldProcess, reason, action based on file history
  - ‚úÖ Processing efficiency tracking and reporting
- [x] **Test Suite Integration**:
  - ‚úÖ Fixed all 35 folder lifecycle tests to work with intelligent scanning
  - ‚úÖ Implemented filesystem mocking to prevent test file reading errors
  - ‚úÖ Created mock file state service ensuring test files get processed appropriately
  - ‚úÖ Resolved TypeScript compilation issues
- [x] **Results**: 
  - System now remembers corrupted files and won't reprocess unchanged content
  - Dramatic efficiency improvement - avoids unnecessary processing
  - Clear logging of processing decisions with reasons
  - Proper handling of file processing outcomes across daemon restarts

### Priority 6: Resource Management ‚úÖ **PARTIALLY COMPLETED (2025-08-09)**
- [x] **Problem**: No limits or throttling between folder operations in daemon
- [x] **Root Cause Analysis**: ResourceManager exists but only used in `multi-folder-indexing.ts`, not integrated with daemon's `MonitoredFoldersOrchestrator`
- [x] **Integration Complete**:
  - ‚úÖ Added ResourceManager to `MonitoredFoldersOrchestrator` constructor with daemon-appropriate limits
  - ‚úÖ Wrapped `addFolder()` operations with resource management (queue + memory limits)
  - ‚úÖ Wrapped `onChangesDetected()` scanning operations with resource management
  - ‚úÖ Added proper resource monitoring with throttling warnings and debug logs
  - ‚úÖ Implemented proper ResourceManager shutdown in `stopAll()` method
  - ‚úÖ Set conservative daemon limits: 2 concurrent ops, 512MB memory, 60% CPU, 20 queue size
- [x] **Remaining Tasks Completed**:
  - ‚úÖ **Add memory usage monitoring integration**: Complete memory monitoring system implemented
    - Added comprehensive memory usage monitoring to `MonitoredFoldersOrchestrator` (10-second intervals)
    - Integrated process memory statistics (heap, RSS, external) with ResourceManager metrics
    - Added intelligent logging thresholds (info for high usage, debug for normal)
    - Implemented automatic garbage collection triggers when memory exceeds 450MB
    - Added memory warnings at 400MB heap usage or 85% heap utilization  
    - Exposed memory statistics via daemon `/status` HTTP endpoint
    - Memory monitoring includes folder counts, resource manager status, and heap utilization percentages
- [x] **Additional Tasks Completed**:
  - ‚úÖ **Create file count limits per batch processing**: File count limiting system implemented
    - Added `getMaxFilesPerBatch()` method with conservative 50-file limit per batch
    - Implemented batch processing in `FolderLifecycleService.processScanResults()` 
    - Added comprehensive logging for batch size warnings when limits exceeded
    - Prevents memory overload from processing too many files simultaneously
    - Logs effective batch sizes and skipped file counts for monitoring
    - Future-ready for configurable limits based on system resources and file sizes
- [x] **Final Task Completed**:
  - ‚úÖ **Implement resource cleanup on errors**: Comprehensive error cleanup system implemented
    - Added `performResourceCleanup()` method in `MonitoredFoldersOrchestrator` for complete resource cleanup
    - Integrated cleanup calls in all major error paths (`addFolder`, `executeAddFolder` operations)
    - Added `performOperationCleanup()` method in `ResourceManager` for failed operation cleanup
    - Resource cleanup includes: operation cancellation, manager stopping, file watcher cleanup, directory cleanup, configuration removal, FMDM updates
    - Automatic garbage collection triggers for failed operations with high memory estimates
    - Comprehensive logging of cleanup activities and resource statistics
    - Error-safe cleanup: cleanup failures don't fail parent operations

## ‚úÖ **PRIORITY 6: RESOURCE MANAGEMENT - FULLY COMPLETED (2025-08-09)**

All resource management components have been successfully implemented:
- **Resource coordination** via ResourceManager integration
- **Memory monitoring** with intelligent thresholds and automatic GC
- **File count limiting** with 50-file batches to prevent overload  
- **Resource cleanup** with comprehensive error recovery

The indexing system now has production-grade resource management that prevents system overload and ensures clean recovery from failures.

### Priority 7: Database Integrity
- [ ] **Problem**: No corruption detection or recovery
- [ ] **Fix**:
  - Add database health checks
  - Implement automatic rebuild on corruption
  - Add transaction support for atomic operations
  - Create backup mechanism

## üìä Success Metrics

The indexing system is production-ready when:
- [ ] Can handle 100,000+ files without crashing
- [ ] Recovers from any single point of failure
- [ ] Provides clear error messages for all failure modes
- [ ] Never loses indexed data due to crashes
- [ ] Handles concurrent operations without deadlocks
- [ ] Respects system resources (memory, CPU, disk)
- [ ] Self-heals from transient failures
- [ ] Maintains data integrity across restarts

## üéØ Execution Plan

### Phase 1: Manual Testing (TMOAT Approach)
- [ ] Run each test scenario manually
- [ ] Document actual vs expected behavior
- [ ] Identify all failure points

### Phase 2: Fix Implementation
- [ ] Fix issues in priority order
- [ ] Each fix includes error handling
- [ ] Add defensive programming

### Phase 3: Automated Test Creation
- [ ] Convert manual tests to automated
- [ ] Add stress tests
- [ ] Add chaos engineering tests

### Phase 4: Production Validation
- [ ] Run on real-world folders (10GB+)
- [ ] Test with various file systems
- [ ] Validate resource usage

## üìù Progress Notes

### Test Suite A Progress: File System Chaos

**Test 1 - Files changing during indexing: üî¥ MAJOR ISSUE FOUND**
- **Issue**: File watching appears to be working (daemon logs show file watcher started), but when files are added/modified/deleted in an active folder, the daemon does not detect the changes and transition back to scanning/indexing.
- **Evidence**: 
  - Created fresh folder with 20 files
  - Added folder to daemon ‚Üí went through scanning ‚Üí indexing ‚Üí active (100%)
  - Added 2 new files, deleted 2 files, modified 1 file after reaching active state
  - Created additional obvious test files (major_change.txt, another_test.txt, obvious_new_file.txt)
  - **Result**: Daemon stayed at "active (100%)" - no state transitions detected
- **Expected**: Should transition active ‚Üí scanning ‚Üí ready ‚Üí indexing ‚Üí active when changes detected
- **Root Cause**: File watcher is started but change detection is not triggering folder state transitions properly

**Test 2 - Folder deletion while active: üî¥ CRITICAL ISSUE FOUND**
- **Issue**: When a monitored folder is deleted from disk while daemon is running, the daemon does not detect this and keeps the folder in FMDM with "active" status
- **Evidence**:
  - Created folder with 5 files
  - Added to daemon ‚Üí scanning ‚Üí indexing ‚Üí active (100%)
  - Deleted entire folder from disk using `rm -rf`
  - **Result**: Folder remained in FMDM with "active (100%)" status
- **Expected**: Folder should be removed from FMDM or marked with error status
- **Root Cause**: No folder existence validation or file watcher error handling for deleted folders

**Critical Issues Summary:**
1. ‚úÖ **FIXED** - File changes in active folders are now properly detected and trigger re-scanning
2. ‚úÖ **FIXED** - Deleted folders are automatically removed from FMDM via periodic validation

**Fixes Implemented:**
- **File Change Detection**: 
  - Added `setChangeDetectionCallback` in `MonitoringOrchestrator` 
  - Connected file watcher events to `MonitoredFoldersOrchestrator.onChangesDetected()`
  - Modified `FolderLifecycleService.startScanning()` to allow transitions from `active` ‚Üí `scanning`
  - File changes now properly trigger: active ‚Üí scanning ‚Üí ready ‚Üí indexing ‚Üí active

- **Folder Deletion Detection**:
  - Added periodic folder validation timer (every 30 seconds) in `MonitoredFoldersOrchestrator`
  - Validates all monitored folders still exist using `fs.existsSync()`
  - Automatically removes non-existent folders from FMDM and configuration
  - Cleans up file watchers and managers for deleted folders

**Additional Production Enhancements:**
- ‚úÖ **Daemon Restart Persistence**: Implemented proper `startAll()` method to restore folders from configuration
- ‚úÖ **Error Folder Reporting**: Enhanced FMDM to track and report error folders separately
- ‚úÖ **Configuration Persistence**: Added automatic saving/removal of folders to/from configuration

**Test Results:**
- ‚úÖ File added to active folder ‚Üí detected within ~2-5 seconds ‚Üí proper re-indexing
- ‚úÖ Folder deleted from disk ‚Üí removed from FMDM within ~30 seconds ‚Üí no ghost folders
- ‚úÖ All existing test suites passing (Python embeddings, performance, user stories, file watching)
- ‚ö†Ô∏è 2 daemon e2e tests still failing (WebSocket FMDM broadcast + error reporting issues)

### Latest Progress (2025-01-07):

#### ‚úÖ **COMPLETED** - Daemon Restart Functionality
- **Issue**: Daemon restart test failing with "FMDM update timeout after 5000ms"
- **Root Cause**: DaemonRegistry singleton enforcement preventing new daemon from starting
- **Fixes Applied**:
  - ‚úÖ Added explicit `updateDaemonStatus()` call to ensure FMDM has daemon info  
  - ‚úÖ Added `--restart` flag to override singleton enforcement
  - ‚úÖ Added 2-second delay after daemon termination to ensure ports are released
  - ‚úÖ Enhanced process management with proper SIGTERM handling
- **Evidence**: Restarted daemon shows proper startup sequence, WebSocket server starts, clients can connect
- **Status**: Daemon restart functionality is working correctly

#### ‚ö†Ô∏è **OPEN ISSUES** - E2E Test Failures
1. **WebSocket Initial FMDM Broadcast Race Condition**:
   - Client connects to restarted daemon successfully
   - Initial FMDM state not sent to newly connected clients
   - Race condition or timing issue in WebSocket message handling

2. **Error Reporting Test Timeout**:
   - "should report errors through FMDM" test failing with 10s timeout
   - Error state not being properly broadcast to WebSocket clients
   - Same underlying WebSocket broadcast issue

#### ‚úÖ **COMPLETED** - Folder Validation Display System
- **Issue**: Deleted folders were not properly shown with error status in TUI
- **Problems Fixed**:
  - Deleted folders were being removed from tracking instead of showing error status
  - Validation error messages not displayed in TUI collapsed/expanded modes
  - Inconsistent status coloring for error states
  - Redundant error status lines in expanded view
  - Terminal resize causing folder manager to collapse unexpectedly
- **Solutions Implemented**:
  - ‚úÖ Modified `MonitoredFoldersOrchestrator.validateAllFolders()` to mark deleted folders with error status instead of removing them
  - ‚úÖ Enhanced `ContainerListItem.tsx` to display validation messages in expanded mode
  - ‚úÖ Fixed `ManageFolderItem.tsx` error status coloring to use proper theme colors
  - ‚úÖ Implemented advanced truncation logic for error status display
  - ‚úÖ Removed redundant "‚óè error" line in expanded mode to prevent duplication
  - ‚úÖ Added expansion state persistence during terminal resizes in `AppFullscreen.tsx`
- **Result**: Deleted folders now show `[error] ‚úó Folder no longer exists` with proper error styling and maintain expanded state across terminal resizes
- **Status**: ‚úÖ **PRODUCTION-READY** - Folder error handling and display is robust and user-friendly

#### ‚úÖ **COMPLETED** - Python Embedding Tests Fixed  
- **Issue**: Python embedding tests were failing due to missing PyTorch and sentence-transformers dependencies
- **Root Cause**: Virtual environment was missing core ML dependencies causing test timeouts with "Request timeout: generate_embeddings (30000ms)"
- **Fixes Applied**:
  - ‚úÖ Recreated Python virtual environment with clean dependency installation
  - ‚úÖ Installed PyTorch 2.8.0 with CPU-only support for M1 Mac compatibility
  - ‚úÖ Installed sentence-transformers 5.1.0 with all required dependencies (scikit-learn, Pillow, etc.)
  - ‚úÖ Resolved sentencepiece build issues by using pre-built wheels and avoiding problematic versions
  - ‚úÖ All Python embedding tests now pass successfully with proper GPU acceleration (MPS)
- **Evidence**: Full test suite passes with 29/29 tests successful, keep-alive functionality verified, embedding generation working with ~40-200ms response times
- **Status**: ‚úÖ **PRODUCTION-READY** - Python embedding service is fully functional with proper dependency management

#### ‚úÖ **FIXED (2025-08-09)** - Daemon Crash Recovery Test Suite
- **Issue**: Automated crash recovery tests were failing with daemon startup and FMDM timeout errors
- **Root Cause**: Configuration contamination between test runs causing daemon to attempt restoration of non-existent folders
- **Solution**: Implemented isolated configuration approach with `FOLDER_MCP_USER_CONFIG_DIR` environment variable
- **Test Enhancement**: Updated test logic to properly verify database persistence in isolated environment
- **Result**: Both crash recovery tests now pass consistently, validating production crash recovery capabilities

#### ‚ö†Ô∏è **REMAINING ISSUE** - Folder Lifecycle State Problem  
- **Issue**: On daemon restart, TUI shows all folders as "indexing (0%)" instead of proper lifecycle
- **Expected Flow**: pending ‚Üí scanning ‚Üí ready ‚Üí indexing ‚Üí active
- **Current Flow**: indexing ‚Üí active (missing intermediate states)
- **Impact**: User experience shows incorrect progress states

**Production Readiness Status:**
üéØ **CORE PRODUCTION ISSUES RESOLVED** - The indexing system handles the two most critical production scenarios (file changes + folder deletion). **Daemon restart functionality is working correctly**. **Folder error handling and display system is production-ready**. **Python embedding service is fully functional**. **Embeddings persistence across daemon restarts is now fixed**. **Crash recovery test suite is fully operational**.

‚úÖ **RECENTLY COMPLETED (2025-08-09)**: 
- **PHASE 2: INTELLIGENT FILE STATE TRACKING SYSTEM COMPLETED**: Addressed the core architectural flaw identified by user
  - **Original Issue**: System doing "dumb change detection" - corrupted files with same hash repeatedly re-processed
  - **User's Key Question**: "the status table should have remembered that the file was corrupted and that it has the same hash that has not changed. so why try and index it again?"
  - **Solution Implemented**: Complete intelligent file state tracking system
    - ‚úÖ `file_states` database table with persistent processing outcomes
    - ‚úÖ FileStateManager domain service making intelligent processing decisions  
    - ‚úÖ Content-based MD5 hashing for accurate change detection
    - ‚úÖ State tracking: pending, processing, indexed, failed, corrupted, skipped
    - ‚úÖ Processing efficiency reporting (% of files avoided unnecessary processing)
  - **Architecture Benefits**: 
    - Corrupted files remembered and skipped until content changes
    - Successfully processed files not reprocessed unless modified
    - Clear audit trail of processing decisions with reasons
    - Dramatic efficiency improvement for large folders
  - **Test Suite**: All 35 folder lifecycle tests passing with intelligent scanning integration
  - **Build Status**: TypeScript compilation successful, production-ready

‚úÖ **COMPLETED (2025-01-08)**: 
- **FIXED STATE PERSISTENCE BUG**: Resolved critical issue where embeddings were re-indexed on every daemon restart
  - Root cause #1: Wrong database initialization method causing potential data wipes
  - Root cause #2: Fingerprints stored as timestamps (`fingerprint_${Date.now()}`) instead of actual file hashes
  - Solution: 
    - Use `loadIndex()` for proper database initialization
    - Store actual MD5 file hashes as fingerprints
    - Correct fingerprint comparison logic for change detection
  - Result: Files with matching hashes are correctly detected as unchanged (requires one-time DB rebuild)
- Enhanced folder validation display system with proper error handling
- Fixed Python embedding tests by resolving PyTorch/sentence-transformers dependency issues

‚ö†Ô∏è **REMAINING ISSUES**: WebSocket broadcast race condition affecting test reliability and user experience. Folder lifecycle state display on daemon restart. These need to be resolved for full production readiness.

*Use this section to track progress, findings, and decisions as we work through the checklist*

---

This plan will make our indexing system truly production-ready through comprehensive testing and systematic fixes.