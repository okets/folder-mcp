# Test System Correction Analysis

## Overview
After the massive architectural changes from our course correction plan (Steps 1-8), we now have 30 failed tests across 8 files that need systematic analysis. This document provides a TMOAT-style analysis of each failing test to determine the best course of action.

## Test Categories and Status

### 1. Architectural Pattern Tests (`tests/architectural/patterns.test.ts`)
**Status**: 2 failures
**Pattern**: Dependency Injection Pattern compliance

#### Analysis: KEEP AND FIX 
**Failure**: `should use dependency injection for service dependencies`
**Root Cause**: Course correction introduced new services (FolderIndexingQueue, UnifiedModelFactory, IEmbeddingModel bridges) that may not follow proper DI patterns.
**Action Required**: Fix the implementation to properly register new services in DI container
**Business Value**: HIGH - Architectural integrity is critical for maintainability

---

### 2. Daemon E2E Integration Tests (`tests/integration/daemon-e2e.test.ts`)
**Status**: 10 failures
**Pattern**: All timeout-related failures

#### Analysis: KEEP BUT MAJOR REFACTOR NEEDED
**Failures**:
- `should complete full folder lifecycle via daemon` - Timeout
- `should detect and process multiple file types` - Timeout  
- `should handle concurrent folder processing` - Timeout (NOW INVALID)
- `should provide real-time progress updates` - Timeout
- `should handle daemon restart with persistent folder state` - Timeout
- `should handle folder removal during processing` - Timeout
- `should detect and handle files removed after indexing` - Timeout
- `should debounce multiple rapid file changes` - Timeout
- `should report errors through FMDM` - Timeout

**Root Cause**: Sequential processing architecture (Step 6) fundamentally changed how the daemon works:
- NO MORE CONCURRENT PROCESSING - folders index one at a time
- Different timing expectations due to model loading/switching
- FMDM state changes are different (queuing, sequential progression)

**Actions Required**:
1. **INVALID TEST**: `should handle concurrent folder processing` - DELETE (contradicts sequential architecture)
2. **UPDATE TIMEOUTS**: Increase timeouts for remaining tests (model loading takes time)
3. **UPDATE EXPECTATIONS**: Tests expecting concurrent behavior need sequential behavior assertions
4. **UPDATE STATE VALIDATION**: FMDM states now include "pending", "queued" statuses

**Business Value**: CRITICAL - These are our primary integration tests

---

### 3. Daemon Crash Recovery Tests (`tests/integration/daemon-crash-recovery.test.ts`)
**Status**: 2 failures
**Pattern**: Daemon startup failures

#### Analysis: KEEP AND FIX
**Failures**:
- `should preserve indexing work after daemon restart` - Daemon startup timeout
- `should handle corrupted database gracefully with restart recovery` - Daemon exited unexpectedly

**Root Cause**: Step 9 issue - daemon takes too long to start up, affecting crash recovery tests
**Action Required**: 
1. Fix daemon startup performance (investigate slow initialization)
2. Update test timeouts if startup optimization isn't possible
3. Ensure crash recovery works with sequential processing

**Business Value**: HIGH - Production stability depends on crash recovery

---

### 4. Daemon Registry Tests (`tests/daemon/registry/daemon-registry.test.ts`)
**Status**: 8 failures
**Pattern**: All fail due to existing daemon process (PID 5442)

#### Analysis: KEEP BUT FIX TEST ISOLATION
**Failures**: All registry tests fail because they can't register due to existing daemon

**Root Cause**: Test environment contamination - daemon from previous test run still running
**Action Required**:
1. Add proper test cleanup - kill any existing daemon processes before tests
2. Add beforeEach/afterEach hooks to ensure clean daemon state
3. Consider using different ports for test daemons vs real daemons

**Business Value**: HIGH - Daemon registry is critical for TUI<->daemon communication

---

### 5. SQLite Vector Integration Tests (`tests/daemon/indexing/sqlite-vec-integration.test.ts`)
**Status**: 1 failure
**Pattern**: Database file not created

#### Analysis: KEEP AND FIX
**Failure**: `should create per-folder databases in correct locations`
**Issue**: `expect(existsSync(dbPath)).toBe(true)` - embeddings.db not created

**Root Cause**: Sequential processing may have changed where/when SQLite databases are created
**Action Required**: Verify SQLite database creation in new architecture, fix path/timing issues

**Business Value**: CRITICAL - Vector storage is core functionality

---

### 6. Configuration Validation Tests (`tests/unit/config/ConfigurationComponent.test.ts`)
**Status**: 1 failure
**Pattern**: Model validation failure

#### Analysis: KEEP AND FIX
**Failure**: `should accept supported embedding models`
**Issue**: Model validation returning false for supported models

**Root Cause**: Step 8 model ID prefix changes (`folder-mcp:` â†’ `gpu:`, `folder-mcp-lite:` â†’ `cpu:`) broke model validation
**Action Required**: Update model validation logic to recognize new prefixes

**Business Value**: MEDIUM - Validation prevents user errors but not core functionality

---

### 7. ONNX Model Bridge Tests (`tests/integration/onnx-model-bridge.test.ts`)
**Status**: 1 failure
**Pattern**: Vector similarity calculation

#### Analysis: KEEP AND FIX
**Failure**: `should calculate similarity between vectors`

**Root Cause**: Step 5 ONNX bridge implementation may have introduced vector calculation issues
**Action Required**: Debug ONNX vector similarity calculation, ensure bridge implements interface correctly

**Business Value**: MEDIUM - ONNX models are alternative to Python models

---

### 8. Performance Tests (`tests/performance/indexing.perf.test.ts`)
**Status**: 8 failures  
**Pattern**: Environment setup issues

#### Analysis: CONDITIONAL - SKIP OR FIX ENVIRONMENT
**Failure**: `TypeError: Cannot read properties of undefined (reading 'cleanup')`
**Issue**: Test environment not properly initialized (`env.cleanup()`)

**Root Cause**: Performance test environment setup broken, possibly due to architectural changes
**Action Required**: 
- **OPTION A**: Fix environment setup if performance tests are critical
- **OPTION B**: Skip/disable performance tests and rely on manual TMOAT verification

**Business Value**: LOW - Nice to have but not blocking core functionality

---

## CRITICAL FINDING: Model ID Mismatch Issue

**ðŸš¨ ROOT CAUSE DISCOVERED**: From live daemon logs during test execution:
```
Error: Model gpu:paraphrase-multilingual-minilm not found in curated models
```

**Analysis**: Step 8 model ID prefix changes (`folder-mcp:` â†’ `gpu:`, `folder-mcp-lite:` â†’ `cpu:`) are complete in the codebase, but:

1. **Tests are using invalid model IDs**: `gpu:paraphrase-multilingual-minilm` doesn't exist in curated models
2. **Curated models file**: Needs verification of actual available model IDs after Step 8 changes
3. **Test model selection**: Tests need to use valid model IDs that actually exist

**Impact**: This single issue is causing cascading failures across ALL daemon E2E tests because folders can't be added due to invalid model IDs.

**Investigation Results**: 
âœ… `gpu:paraphrase-multilingual-minilm` IS in curated-models.json (line 214)  
âŒ BUT the daemon can't find it: `Model gpu:paraphrase-multilingual-minilm not found in curated models`

**Root Cause**: Model loading/lookup mechanism is broken. Possibilities:
1. Model registry/loader not reading curated-models.json correctly
2. Model lookup logic has bugs introduced during Step 8 refactoring  
3. File path issues or JSON parsing problems

**Immediate Fix Required**: 
1. Debug model lookup mechanism in daemon/orchestrator
2. Fix model loading to properly read curated-models.json
3. This will likely fix 80% of the failing tests immediately

**Available Model IDs for Tests**:
- GPU Models: `gpu:bge-m3`, `gpu:multilingual-e5-large`, `gpu:paraphrase-multilingual-minilm`
- CPU Models: `cpu:xenova-multilingual-e5-small`, `cpu:xenova-multilingual-e5-large`

## Test Strategy Recommendations

### Immediate Actions (Unblock Development)

1. **Kill existing daemon processes**: `pkill -f "folder-mcp.*daemon"`
2. **Skip performance tests temporarily**: Add `.skip()` to performance tests
3. **Fix DI registration**: Register new services properly in container
4. **Update model validation**: Support new model ID prefixes

### Medium-term Fixes (1-2 days)

1. **Refactor daemon E2E tests**: Update for sequential processing, remove concurrent test
2. **Fix daemon startup**: Investigate Step 9 slow startup issue
3. **Fix SQLite integration**: Ensure databases created with new architecture
4. **Fix test isolation**: Add proper setup/teardown for daemon tests

### Strategic Questions

1. **Performance Tests**: Keep or remove? Manual TMOAT testing might be sufficient
2. **Test Timeouts**: How much longer should tests wait given sequential processing?
3. **ONNX Bridge**: Is this critical for Phase 8 or can we focus on Python models?

## Suggested New Tests (Based on Course Correction Steps)

Given the architectural changes, we need NEW tests to cover our new capabilities:

### 1. Sequential Processing Tests
```typescript
// Test that only one folder indexes at a time
describe('Sequential Folder Processing', () => {
  it('should process folders one at a time');
  it('should show pending status for queued folders');
  it('should handle queue when folders added/removed');
});
```

### 2. Model Bridge Tests  
```typescript
// Test unified model interface
describe('Model Bridge Integration', () => {
  it('should switch between Python and ONNX models');
  it('should pause/resume indexing for semantic search');
  it('should handle model loading failures gracefully');
});
```

### 3. Priority Queue Tests
```typescript
// Test semantic search interruption
describe('Priority Processing', () => {
  it('should interrupt indexing for immediate search requests');
  it('should resume indexing after priority request completes');
  it('should handle model switching during priority requests');
});
```

### 4. FMDM State Tests
```typescript
// Test new folder states from sequential processing
describe('FMDM State Management', () => {
  it('should show correct queue positions');
  it('should update states during sequential processing');
  it('should handle folder removal from queue');
});
```

## Success Metrics

To know when our test system is healthy:

1. **Zero test environment contamination**: All tests run clean from fresh state
2. **Accurate business logic coverage**: Tests verify sequential processing correctly  
3. **Proper timeout values**: Tests don't fail due to new architectural timing
4. **New architecture coverage**: Tests verify unified model interface and priority queues
5. **TMOAT complementarity**: Automated tests cover unit logic, TMOAT covers end-to-end flows

## Implementation Priority

**Phase 1 (Immediate)**:
- Kill daemon processes and fix test isolation
- Skip performance tests temporarily  
- Fix DI pattern compliance
- Update model validation for new prefixes

**Phase 2 (This week)**:
- Refactor daemon E2E tests for sequential processing
- Fix SQLite database creation
- Add new sequential processing tests
- Fix daemon startup performance

**Phase 3 (Next week)**:  
- Decide on performance tests (fix or remove)
- Add comprehensive FMDM state tests
- Add priority queue interruption tests
- Polish ONNX bridge if needed

This systematic approach will restore test system health while ensuring our new sequential processing architecture is properly validated.

---

## Comprehensive Suggested Tests Based on Course Correction Steps

Based on our completed course correction Steps 1-8, we need new tests to validate the architecture changes:

### 1. Model System Tests (Steps 3-5, 8)

#### Unified Model Interface Tests
```typescript
describe('UnifiedModelFactory', () => {
  it('should create Python model bridges for gpu: prefixed models');
  it('should create ONNX model bridges for cpu: prefixed models');
  it('should reject invalid model IDs with clear error messages');
  it('should handle model loading failures gracefully');
});

describe('Model Bridge Implementations', () => {
  it('should implement IEmbeddingModel interface consistently');
  it('should handle immediate=true flag for priority processing');
  it('should properly load/unload models to free resources');
  it('should return embeddings in consistent format across bridges');
});
```

#### Model ID Validation Tests
```typescript
describe('Model ID Validation (Step 8)', () => {
  it('should validate gpu: prefixed models exist in curated list');
  it('should validate cpu: prefixed models exist in curated list');  
  it('should reject old folder-mcp: prefixed model IDs');
  it('should provide helpful error messages for invalid models');
});
```

### 2. Sequential Processing Tests (Step 6)

#### Folder Indexing Queue Tests
```typescript
describe('FolderIndexingQueue', () => {
  it('should process folders one at a time in FIFO order');
  it('should show "pending" status for queued folders');
  it('should handle folder removal from active queue');
  it('should handle folder addition while another is processing');
  it('should maintain queue state through daemon restart');
});

describe('Sequential Processing Validation', () => {
  it('should never index two folders simultaneously');
  it('should load/unload models between different folder types');
  it('should update FMDM states correctly during queue processing');
  it('should handle queue exhaustion gracefully');
});
```

### 3. Priority System Tests (Step 7)

#### Semantic Search Priority Tests
```typescript
describe('Priority Processing System', () => {
  it('should pause folder indexing for immediate search requests');
  it('should resume indexing after priority request completes');
  it('should switch models when search targets different folder type');
  it('should maintain search response time under 2 seconds');
  it('should handle multiple concurrent search requests');
});

describe('Model Switching During Priority', () => {
  it('should pause GPU indexing â†’ load CPU model â†’ search â†’ resume GPU');
  it('should handle model switching failures gracefully');
  it('should maintain embeddings consistency across model switches');
  it('should log model switch operations for debugging');
});
```

### 4. FMDM State Management Tests

#### New State Support Tests
```typescript
describe('FMDM State Management', () => {
  it('should show "queued" status for folders waiting in line');
  it('should show "switching" status during model transitions');
  it('should show accurate queue position for pending folders');
  it('should update progress correctly during sequential processing');
  it('should handle state transitions during daemon restart');
});

describe('WebSocket State Broadcasting', () => {
  it('should broadcast queue updates to all connected TUI clients');
  it('should handle TUI client disconnection/reconnection gracefully');
  it('should maintain state consistency across multiple clients');
});
```

### 5. Resource Management Tests

#### Memory and Cleanup Tests
```typescript
describe('Resource Management', () => {
  it('should free model memory when switching between folder types');
  it('should maintain stable memory usage during sequential processing');
  it('should handle out-of-memory conditions during model loading');
  it('should clean up resources when folders are removed');
});

describe('Database Management', () => {
  it('should create per-folder SQLite databases correctly');
  it('should NOT create duplicate metadata JSON files (Step 10)');
  it('should handle database corruption gracefully');
  it('should maintain vector storage consistency');
});
```

### 6. Error Handling and Edge Cases

#### Robustness Tests
```typescript
describe('Error Handling', () => {
  it('should handle model download failures during indexing');
  it('should skip corrupted folders and continue queue processing');
  it('should handle Python service crashes during embeddings');
  it('should recover from ONNX model loading errors');
  it('should handle file system permission errors');
});

describe('Edge Cases', () => {
  it('should handle daemon restart with active indexing queue');
  it('should handle rapid folder add/remove operations');
  it('should handle model files being deleted during operation');
  it('should handle network issues during model downloads');
});
```

### 7. Integration with Existing Systems

#### MCP Protocol Tests
```typescript
describe('MCP Integration', () => {
  it('should maintain search endpoint functionality with priority system');
  it('should handle folder lifecycle operations through MCP');
  it('should provide accurate folder status through MCP endpoints');
});

describe('Configuration Integration', () => {
  it('should respect model selection from configuration hierarchy');
  it('should handle configuration changes during operation');
  it('should validate model availability against configuration');
});
```

### 8. Performance and Timing Tests

#### Sequential Performance Tests
```typescript
describe('Sequential Processing Performance', () => {
  it('should start next folder within 5 seconds of previous completion');
  it('should maintain search response time under 2s during indexing');
  it('should handle large queues without memory leaks');
  it('should provide accurate progress reporting for all queued folders');
});
```

### 9. Manual TMOAT Integration Tests

Tests that should be validated through TMOAT scripts:

```typescript
describe('TMOAT Validation Tests', () => {
  it('should complete end-to-end folder lifecycle with GPU model');
  it('should complete end-to-end folder lifecycle with CPU model');
  it('should handle mixed GPU/CPU folder queue correctly');
  it('should maintain TUI responsiveness during heavy processing');
  it('should handle daemon restart with folder persistence');
});
```

## Test Priority Implementation Order

**Phase 1 (Fix Immediate Breakage)**:
1. Model lookup and validation tests
2. Basic sequential processing tests  
3. Fix existing daemon E2E test timeouts

**Phase 2 (Validate New Architecture)**:
1. Priority system and model switching tests
2. FMDM state management tests
3. Resource management tests

**Phase 3 (Polish and Edge Cases)**:
1. Error handling and robustness tests
2. Performance optimization tests
3. TMOAT integration validation

This test suite will provide comprehensive coverage of our new sequential processing architecture while maintaining compatibility with existing functionality.